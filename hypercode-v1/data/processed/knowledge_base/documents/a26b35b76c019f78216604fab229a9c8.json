{
  "file_name": "test_framework.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\duelcode\\test_framework.py",
  "file_size": 10922,
  "created": "2025-11-23T11:56:45.661746",
  "modified": "2025-11-23T11:56:45.661746",
  "file_type": "code",
  "content_hash": "d8fa9b2208ee082c28ebd66022ff7fc1",
  "content_type": "text",
  "content": "#!/usr/bin/env python3\n\"\"\"\nAutomated Testing Suite for DuelCode Framework\nTests tutorials against validation rules and checks framework integrity.\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nfrom dataclasses import asdict, dataclass\nfrom enum import Enum\nfrom pathlib import Path\nfrom subprocess import CompletedProcess\nfrom typing import Dict, List\n\n\nclass TestResult(Enum):\n    PASSED = \"PASSED\"\n    FAILED = \"FAILED\"\n    SKIPPED = \"SKIPPED\"\n\n\n@dataclass\nclass TestCase:\n    name: str\n    file_path: str\n    expected_errors: int = 0\n    expected_warnings: int = 0\n    expected_info: int = 0\n    description: str = \"\"\n\n\n@dataclass\nclass TestRun:\n    test_case: TestCase\n    result: TestResult\n    actual_errors: int\n    actual_warnings: int\n    actual_info: int\n    output: str\n    duration: float\n\n\nclass DuelCodeTestSuite:\n    def __init__(self, duelcode_dir: str = \"DuelCode\"):\n        self.duelcode_dir = Path(duelcode_dir)\n        self.test_results: List[TestRun] = []\n\n    def discover_tutorials(self) -> List[Path]:\n        \"\"\"Find all tutorial markdown files.\"\"\"\n        tutorials: List[Path] = []\n        for pattern in [\"tutorial-*.md\", \"*tutorial*.md\"]:\n            tutorials.extend(self.duelcode_dir.glob(pattern))\n        return sorted(tutorials)\n\n    def run_validator(self, file_path: Path) -> CompletedProcess[str]:\n        \"\"\"Run the ultra validator on a file.\"\"\"\n        validator_path = self.duelcode_dir / \"ultra_validator.py\"\n        cmd = [sys.executable, str(validator_path), str(file_path)]\n\n        try:\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n            return result\n        except subprocess.TimeoutExpired:\n            return subprocess.CompletedProcess(cmd, -1, \"\", \"Validator timed out\")\n        except Exception as e:\n            return subprocess.CompletedProcess(\n                cmd, -1, \"\", f\"Error running validator: {e}\"\n            )\n\n    def parse_validator_output(self, output: str) -> Dict[str, int]:\n        \"\"\"Parse validator output to count issues by severity.\"\"\"\n        counts = {\"errors\": 0, \"warnings\": 0, \"info\": 0}\n\n        for line in output.split(\"\\n\"):\n            line = line.strip().upper()\n            if \"ERROR\" in line and \"ERRORS\" in line:\n                # Extract number from \"ERRORS (X):\"\n                import re\n\n                match = re.search(r\"ERRORS\\s*\\((\\d+)\\)\", line)\n                if match:\n                    counts[\"errors\"] = int(match.group(1))\n            elif \"WARNING\" in line and \"WARNINGS\" in line:\n                import re\n\n                match = re.search(r\"WARNINGS\\s*\\((\\d+)\\)\", line)\n                if match:\n                    counts[\"warnings\"] = int(match.group(1))\n            elif \"INFO\" in line and \"INFO\" in line:\n                import re\n\n                match = re.search(r\"INFO\\s*\\((\\d+)\\)\", line)\n                if match:\n                    counts[\"info\"] = int(match.group(1))\n\n        return counts\n\n    def test_tutorial(self, tutorial_path: Path) -> TestRun:\n        \"\"\"Test a single tutorial file.\"\"\"\n        test_case = TestCase(\n            name=tutorial_path.name,\n            file_path=str(tutorial_path),\n            description=f\"Testing {tutorial_path.name}\",\n        )\n\n        print(f\"Testing: {test_case.name}...\", end=\" \")\n\n        # Run validator\n        import time\n\n        start_time = time.time()\n        result = self.run_validator(tutorial_path)\n        duration = time.time() - start_time\n\n        # Parse output\n        counts = self.parse_validator_output(result.stdout)\n\n        # Determine if test passed (no errors)\n        test_result = TestResult.PASSED if counts[\"errors\"] == 0 else TestResult.FAILED\n\n        test_run = TestRun(\n            test_case=test_case,\n            result=test_result,\n            actual_errors=counts[\"errors\"],\n            actual_warnings=counts[\"warnings\"],\n            actual_info=counts[\"info\"],\n            output=result.stdout,\n            duration=duration,\n        )\n\n        print(f\"{test_result.value} ({duration:.2f}s)\")\n        return test_run\n\n    def test_validator_integrity(self) -> TestRun:\n        \"\"\"Test that validator itself is working correctly.\"\"\"\n        test_case = TestCase(\n            name=\"Validator Integrity Test\",\n            file_path=\"self_test\",\n            description=\"Test validator can run without errors\",\n        )\n\n        print(\"Testing validator integrity...\", end=\" \")\n\n        try:\n            # Test with a known good file\n            good_file = self.duelcode_dir / \"tutorial-01-hello-world.md\"\n            if not good_file.exists():\n                test_result = TestResult.SKIPPED\n                output = \"No test file available\"\n            else:\n                result = self.run_validator(good_file)\n                if result.returncode == 0:\n                    test_result = TestResult.PASSED\n                    output = \"Validator runs successfully\"\n                else:\n                    test_result = TestResult.FAILED\n                    output = result.stderr\n        except Exception as e:\n            test_result = TestResult.FAILED\n            output = str(e)\n\n        test_run = TestRun(\n            test_case=test_case,\n            result=test_result,\n            actual_errors=0,\n            actual_warnings=0,\n            actual_info=0,\n            output=output,\n            duration=0,\n        )\n\n        print(test_result.value)\n        return test_run\n\n    def test_template_validity(self) -> List[TestRun]:\n        \"\"\"Test that templates pass validation.\"\"\"\n        template_runs = []\n        templates = [\n            self.duelcode_dir / \"TEMPLATE-TUTORIAL-ADVANCED.md\",\n            self.duelcode_dir / \"TEMPLATE-TUTORIAL-QUICK.md\",\n            self.duelcode_dir / \"TEMPLATE-REFERENCE.md\",\n        ]\n\n        for template in templates:\n            if not template.exists():\n                continue\n\n            test_case = TestCase(\n                name=f\"Template Test: {template.name}\",\n                file_path=str(template),\n                description=f\"Testing template {template.name}\",\n            )\n\n            print(f\"Testing template: {template.name}...\", end=\" \")\n\n            # Templates should have minimal issues (mostly info/suggestions)\n            result = self.run_validator(template)\n            counts = self.parse_validator_output(result.stdout)\n\n            # Templates pass if no errors (warnings/info are OK for templates)\n            test_result = (\n                TestResult.PASSED if counts[\"errors\"] == 0 else TestResult.FAILED\n            )\n\n            test_run = TestRun(\n                test_case=test_case,\n                result=test_result,\n                actual_errors=counts[\"errors\"],\n                actual_warnings=counts[\"warnings\"],\n                actual_info=counts[\"info\"],\n                output=result.stdout,\n                duration=0,\n            )\n\n            print(test_result.value)\n            template_runs.append(test_run)\n\n        return template_runs\n\n    def run_all_tests(self) -> None:\n        \"\"\"Run the complete test suite.\"\"\"\n        print(\"=\" * 60)\n        print(\"DuelCode Framework Test Suite\")\n        print(\"=\" * 60)\n\n        # Test validator integrity\n        self.test_results.append(self.test_validator_integrity())\n\n        # Test templates\n        self.test_results.extend(self.test_template_validity())\n\n        # Test all tutorials\n        tutorials = self.discover_tutorials()\n        if not tutorials:\n            print(\"No tutorial files found!\")\n            return\n\n        print(f\"\\nFound {len(tutorials)} tutorial(s)\")\n        print(\"-\" * 40)\n\n        for tutorial in tutorials:\n            self.test_results.append(self.test_tutorial(tutorial))\n\n        # Generate report\n        self.generate_report()\n\n    def generate_report(self) -> None:\n        \"\"\"Generate a detailed test report.\"\"\"\n        print(\"\\n\" + \"=\" * 60)\n        print(\"TEST REPORT\")\n        print(\"=\" * 60)\n\n        passed = sum(1 for r in self.test_results if r.result == TestResult.PASSED)\n        failed = sum(1 for r in self.test_results if r.result == TestResult.FAILED)\n        skipped = sum(1 for r in self.test_results if r.result == TestResult.SKIPPED)\n        total = len(self.test_results)\n\n        print(f\"Total Tests: {total}\")\n        print(f\"Passed: {passed} ✅\")\n        print(f\"Failed: {failed} ❌\")\n        print(f\"Skipped: {skipped} ⏭️\")\n        print(f\"Success Rate: {(passed/total*100):.1f}%\")\n\n        if failed > 0:\n            print(\"\\nFAILED TESTS:\")\n            print(\"-\" * 40)\n            for result in self.test_results:\n                if result.result == TestResult.FAILED:\n                    print(f\"❌ {result.test_case.name}\")\n                    print(f\"   Errors: {result.actual_errors}\")\n                    print(f\"   Warnings: {result.actual_warnings}\")\n                    if result.output:\n                        print(f\"   Details: {result.output[:100]}...\")\n\n        # Save detailed report\n        report_data = {\n            \"summary\": {\n                \"total\": total,\n                \"passed\": passed,\n                \"failed\": failed,\n                \"skipped\": skipped,\n                \"success_rate\": passed / total * 100,\n            },\n            \"tests\": [asdict(r) for r in self.test_results],\n        }\n\n        report_file = self.duelcode_dir / \"test-report.json\"\n        with Path(report_file).open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(report_data, f, indent=2, default=str)\n\n        print(f\"\\nDetailed report saved to: {report_file}\")\n\n\ndef main() -> None:\n    \"\"\"Main entry point.\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"DuelCode Framework Test Suite\")\n    parser.add_argument(\n        \"--duelcode-dir\", default=\"DuelCode\", help=\"Path to DuelCode directory\"\n    )\n    parser.add_argument(\"--file\", help=\"Test specific file only\")\n\n    args = parser.parse_args()\n\n    suite = DuelCodeTestSuite(args.duelcode_dir)\n\n    if args.file:\n        # Test single file\n        file_path = Path(args.file)\n        if file_path.exists():\n            result = suite.test_tutorial(file_path)\n            suite.test_results = [result]\n            suite.generate_report()\n        else:\n            print(f\"File not found: {args.file}\")\n            sys.exit(1)\n    else:\n        # Run full suite\n        suite.run_all_tests()\n\n    # Exit with error code if any tests failed\n    failed_count = sum(1 for r in suite.test_results if r.result == TestResult.FAILED)\n    sys.exit(1 if failed_count > 0 else 0)\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "metadata": {},
  "relative_path": "src\\duelcode\\test_framework.py",
  "id": "a26b35b76c019f78216604fab229a9c8"
}