{
  "file_name": "hypercode_lexer_fixed.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\src\\hypercode_lexer_fixed.py",
  "file_size": 14785,
  "created": "2025-11-15T15:54:41.052530",
  "modified": "2025-11-17T17:48:31.577620",
  "file_type": "code",
  "content_hash": "7fcf4cf0d3f86cb2ab1d0a26d9ccb9c0",
  "content_type": "text",
  "content": "\"\"\"\nHyperCode Lexer - Fixed String Handling with Escape Sequences\nProperly handles escaped quotes, newlines, tabs, and other escape sequences\n\nKey improvements:\n1. Correctly parses escaped characters within strings\n2. Handles both single and double quotes\n3. Preserves or converts escapes (configurable)\n4. Provides detailed error messages for malformed strings\n5. Includes comprehensive test cases\n\"\"\"\n\nimport sys\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import List\n\n\nclass TokenType(Enum):\n    \"\"\"HyperCode token types\"\"\"\n\n    # Core operations\n    PUSH = \"PUSH\"  # >\n    POP = \"POP\"  # <\n    INCR = \"INCR\"  # +\n    DECR = \"DECR\"  # -\n    OUTPUT = \"OUTPUT\"  # .\n    INPUT = \"INPUT\"  # ,\n    LOOP_START = \"LOOP_START\"  # [\n    LOOP_END = \"LOOP_END\"  # ]\n\n    # String literals\n    STRING = \"STRING\"  # \"...\" or '...'\n\n    # Comments\n    COMMENT = \"COMMENT\"  # ;\n\n    # Special\n    EOF = \"EOF\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n@dataclass\nclass Token:\n    \"\"\"Represents a single token with position tracking\"\"\"\n\n    type: TokenType\n    value: str\n    raw_value: str  # Original value (with escapes)\n    position: int\n    line: int\n    column: int\n\n    def __repr__(self) -> str:\n        \"\"\"Readable representation\"\"\"\n        if self.value != self.raw_value:\n            return (\n                f\"Token({self.type.value:15} | value='{self.value}' \"\n                f\"| raw='{self.raw_value}' | L{self.line}:C{self.column})\"\n            )\n        else:\n            return (\n                f\"Token({self.type.value:15} | '{self.value}' \"\n                f\"| L{self.line}:C{self.column})\"\n            )\n\n\nclass LexerError(Exception):\n    \"\"\"Lexer error with context\"\"\"\n\n    def __init__(self, message: str, line: int, column: int, context: str = \"\") -> None:\n        self.message = message\n        self.line = line\n        self.column = column\n        self.context = context\n\n        error_msg = f\"Line {line}, Col {column}: {message}\"\n        if context:\n            error_msg += f\"\\n  Context: {context}\"\n\n        super().__init__(error_msg)\n\n\nclass HyperCodeLexerFixed:\n    \"\"\"\n    Fixed lexer with proper string escape sequence handling.\n\n    Escape sequences supported:\n    - \\\\\" = escaped double quote\n    - \\\\' = escaped single quote\n    - \\\\\\\\ = escaped backslash\n    - \\\\n = newline\n    - \\\\t = tab\n    - \\\\r = carriage return\n    - \\\\b = backspace\n    - \\\\f = form feed\n    \"\"\"\n\n    # Escape sequence mappings\n    ESCAPE_SEQUENCES = {\n        '\"': '\"',  # Escaped double quote\n        \"'\": \"'\",  # Escaped single quote\n        \"\\\\\": \"\\\\\",  # Escaped backslash\n        \"n\": \"\\n\",  # Newline\n        \"t\": \"\\t\",  # Tab\n        \"r\": \"\\r\",  # Carriage return\n        \"b\": \"\\b\",  # Backspace\n        \"f\": \"\\f\",  # Form feed\n        \"0\": \"\\0\",  # Null character\n    }\n\n    # Single character operations\n    OPERATIONS = {\n        \">\": TokenType.PUSH,\n        \"<\": TokenType.POP,\n        \"+\": TokenType.INCR,\n        \"-\": TokenType.DECR,\n        \".\": TokenType.OUTPUT,\n        \",\": TokenType.INPUT,\n        \"[\": TokenType.LOOP_START,\n        \"]\": TokenType.LOOP_END,\n        \";\": TokenType.COMMENT,\n    }\n\n    def __init__(\n        self, source: str, filename: str = \"<stdin>\", preserve_escapes: bool = False\n    ):\n        \"\"\"\n        Initialize lexer.\n\n        Args:\n            source: Source code\n            filename: Filename for error reporting\n            preserve_escapes: If True, keep escapes in output (e.g., \"hello\\\\\"world\")\n                            If False, convert to actual chars (e.g., \"hello\"world\")\n        \"\"\"\n        self.source = source\n        self.filename = filename\n        self.preserve_escapes = preserve_escapes\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n\n    def tokenize(self) -> List[Token]:\n        \"\"\"\n        Convert source to token stream.\n\n        Returns:\n            List of tokens\n\n        Raises:\n            LexerError: On invalid syntax\n        \"\"\"\n        self.tokens = []\n\n        while self.position < len(self.source):\n            char = self.source[self.position]\n\n            # Skip whitespace\n            if char.isspace():\n                self._advance(char)\n                continue\n\n            # Comments (skip until end of line)\n            if char == \";\":\n                self._skip_comment()\n                continue\n\n            # String literals (double quotes)\n            if char == '\"':\n                self._parse_string('\"')\n                continue\n\n            # String literals (single quotes)\n            if char == \"'\":\n                self._parse_string(\"'\")\n                continue\n\n            # Operations\n            if char in self.OPERATIONS:\n                token_type = self.OPERATIONS[char]\n                token = Token(\n                    type=token_type,\n                    value=char,\n                    raw_value=char,\n                    position=self.position,\n                    line=self.line,\n                    column=self.column,\n                )\n                self.tokens.append(token)\n                self._advance(char)\n                continue\n\n            # Unknown character\n            raise LexerError(\n                f\"Unexpected character: '{char}' (ASCII {ord(char)})\",\n                self.line,\n                self.column,\n                \"Valid: > < + - . , [ ] ; or string literals\",\n            )\n\n        # EOF token\n        self.tokens.append(\n            Token(\n                type=TokenType.EOF,\n                value=\"\",\n                raw_value=\"\",\n                position=self.position,\n                line=self.line,\n                column=self.column,\n            )\n        )\n\n        return self.tokens\n\n    def _parse_string(self, quote_char: str) -> None:\n        \"\"\"\n        Parse string literal with escape sequence handling.\n\n        Args:\n            quote_char: Quote character (\" or ')\n\n        Raises:\n            LexerError: If string is unterminated or contains invalid escapes\n        \"\"\"\n        start_pos = self.position\n        start_line = self.line\n        start_col = self.column\n\n        raw_string = \"\"  # Keep original with escapes\n        parsed_string = \"\"  # Convert escapes\n\n        self._advance(quote_char)  # Skip opening quote\n\n        while self.position < len(self.source):\n            char = self.source[self.position]\n\n            # Closing quote (unescaped)\n            if char == quote_char:\n                self._advance(char)\n\n                # Create token\n                token = Token(\n                    type=TokenType.STRING,\n                    value=parsed_string,\n                    raw_value=raw_string,\n                    position=start_pos,\n                    line=start_line,\n                    column=start_col,\n                )\n                self.tokens.append(token)\n                return\n\n            # Escape sequence\n            if char == \"\\\\\":\n                raw_string += char\n                self._advance(char)\n\n                if self.position >= len(self.source):\n                    raise LexerError(\n                        \"Unterminated string: backslash at end of file\",\n                        self.line,\n                        self.column,\n                        f\"String started at line {start_line}, col {start_col}\",\n                    )\n\n                next_char = self.source[self.position]\n\n                # Valid escape sequence?\n                if next_char in self.ESCAPE_SEQUENCES:\n                    escaped_char = self.ESCAPE_SEQUENCES[next_char]\n                    raw_string += next_char\n                    parsed_string += escaped_char\n                    self._advance(next_char)\n                else:\n                    raise LexerError(\n                        f\"Invalid escape sequence: '\\\\{next_char}'\",\n                        self.line,\n                        self.column,\n                        \"Valid escapes: \"\n                        + \", \".join([\"\\\\\" + k for k in self.ESCAPE_SEQUENCES]),\n                    )\n\n                continue\n\n            # Regular character\n            raw_string += char\n            parsed_string += char\n            self._advance(char)\n\n        # Unterminated string\n        raise LexerError(\n            f\"Unterminated string: missing closing {quote_char}\",\n            start_line,\n            start_col,\n            f\"String started here, expected {quote_char} before EOF\",\n        )\n\n    def _skip_comment(self) -> None:\n        \"\"\"Skip comment until end of line\"\"\"\n        while self.position < len(self.source) and self.source[self.position] != \"\\n\":\n            self._advance(self.source[self.position])\n\n    def _advance(self, char: str) -> None:\n        \"\"\"Update position after processing character\"\"\"\n        self.position += 1\n\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n\n    def print_tokens(self, verbose: bool = True) -> None:\n        \"\"\"Print tokens in readable format\"\"\"\n        for token in self.tokens:\n            if verbose:\n                print(token)\n            else:\n                print(f\"{token.type.value:15} | {repr(token.value)}\")\n\n\n# ============================================================================\n# TEST SUITE\n# ============================================================================\n\n\ndef run_tests() -> None:\n    \"\"\"Comprehensive test suite\"\"\"\n\n    test_cases = [\n        # (description, source, expected_tokens)\n        (\"Simple double-quoted string\", '\"hello\"', [(\"STRING\", \"hello\")]),\n        (\"Simple single-quoted string\", \"'world'\", [(\"STRING\", \"world\")]),\n        (\n            \"Multiple strings\",\n            \"\\\"hello\\\" 'world'\",\n            [(\"STRING\", \"hello\"), (\"STRING\", \"world\")],\n        ),\n        (\n            \"String with escaped double quote\",\n            '\"escaped \\\\\"quote\\\\\"\"',\n            [(\"STRING\", 'escaped \"quote\"')],\n        ),\n        (\"String with escaped single quote\", \"'it\\\\'s'\", [(\"STRING\", \"it's\")]),\n        (\n            \"String with escaped backslash\",\n            '\"path\\\\\\\\to\\\\\\\\file\"',\n            [(\"STRING\", \"path\\\\to\\\\file\")],\n        ),\n        (\"String with newline escape\", '\"hello\\\\nworld\"', [(\"STRING\", \"hello\\nworld\")]),\n        (\n            \"String with tab escape\",\n            '\"column1\\\\tcolumn2\"',\n            [(\"STRING\", \"column1\\tcolumn2\")],\n        ),\n        (\n            \"String with multiple escapes\",\n            '\"say \\\\\"hello\\\\nworld\\\\\"\"',\n            [(\"STRING\", 'say \"hello\\nworld\"')],\n        ),\n        (\n            \"HyperCode with strings\",\n            '+++. \"output\"',\n            [\n                (\"INCR\", \"+\"),\n                (\"INCR\", \"+\"),\n                (\"INCR\", \"+\"),\n                (\"OUTPUT\", \".\"),\n                (\"STRING\", \"output\"),\n            ],\n        ),\n        (\n            \"Complex mixed case\",\n            '\"escaped \\\\\"quote\\\\\"\" \\'world\\' > < +',\n            [\n                (\"STRING\", 'escaped \"quote\"'),\n                (\"STRING\", \"world\"),\n                (\"PUSH\", \">\"),\n                (\"POP\", \"<\"),\n                (\"INCR\", \"+\"),\n            ],\n        ),\n    ]\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"üß™ HYPERCODE LEXER - STRING HANDLING TESTS\")\n    print(\"=\" * 70 + \"\\n\")\n\n    passed = 0\n    failed = 0\n\n    for description, source, expected in test_cases:\n        try:\n            lexer = HyperCodeLexerFixed(source)\n            tokens = lexer.tokenize()\n\n            # Filter out EOF token for comparison\n            tokens = [t for t in tokens if t.type != TokenType.EOF]\n\n            # Check if token count matches\n            if len(tokens) != len(expected):\n                print(f\"‚ùå {description}\")\n                print(f\"   Input: {source}\")\n                print(f\"   Expected {len(expected)} tokens, got {len(tokens)}\")\n                print(f\"   Got: {[(t.type.value, repr(t.value)) for t in tokens]}\")\n                failed += 1\n                continue\n\n            # Check each token\n            all_match = True\n            for token, (exp_type, exp_value) in zip(tokens, expected):\n                if token.type.value != exp_type or token.value != exp_value:\n                    all_match = False\n                    break\n\n            if all_match:\n                print(f\"‚úÖ {description}\")\n                passed += 1\n            else:\n                print(f\"‚ùå {description}\")\n                print(f\"   Input: {source}\")\n                print(f\"   Expected: {expected}\")\n                print(f\"   Got: {[(t.type.value, repr(t.value)) for t in tokens]}\")\n                failed += 1\n\n        except LexerError as e:\n            print(f\"‚ùå {description}\")\n            print(f\"   Input: {source}\")\n            print(f\"   Error: {e}\")\n            failed += 1\n\n        except Exception as e:\n            print(f\"‚ùå {description}\")\n            print(f\"   Input: {source}\")\n            print(f\"   Unexpected error: {e}\")\n            failed += 1\n\n    print()\n    print(\"=\" * 70)\n    print(f\"Results: {passed} passed, {failed} failed\")\n    print(\"=\" * 70 + \"\\n\")\n\n\ndef main() -> None:\n    \"\"\"Main entry point\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"HyperCode Lexer - Fixed String Handling\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python lexer_fixed.py --test\n  python lexer_fixed.py '\"hello\\\\\"world\"'\n  echo 'echo \"test\"' | python lexer_fixed.py -\n        \"\"\",\n    )\n\n    parser.add_argument(\"input\", nargs=\"?\", help=\"Input to tokenize\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Run test suite\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n\n    args = parser.parse_args()\n\n    if args.test:\n        run_tests()\n\n    if not args.input:\n        parser.print_help()\n        return\n\n    # Read input\n    source = sys.stdin.read() if args.input == \"-\" else args.input\n\n    try:\n        lexer = HyperCodeLexerFixed(source)\n        tokens = lexer.tokenize()\n\n        print(\"‚úÖ Successfully tokenized\")\n        print(f\"   Total tokens: {len(tokens) - 1}\\n\")\n\n        lexer.print_tokens(verbose=args.verbose)\n\n    except LexerError as e:\n        print(f\"‚ùå Lexer Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\src\\hypercode_lexer_fixed.py",
  "id": "14916463f84816c2c6b2ce16abed5a20"
}