{
  "file_name": "run_lexer.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\scripts\\run_lexer.py",
  "file_size": 8214,
  "created": "2025-11-29T01:56:39.703841",
  "modified": "2025-12-03T00:32:44.645954",
  "file_type": "code",
  "content_hash": "bc08ac8fe773c180798cd238e929ea76",
  "content_type": "text",
  "content": "\"\"\"\nTest suite for HyperCode lexer.\n\nThis module contains comprehensive tests for the HyperCode lexer,\nverifying tokenization of various language constructs and edge cases.\n\"\"\"\n\nimport unittest\nfrom typing import List, TypeVar\n\nfrom hypercode.core.lexer import Lexer\nfrom hypercode.core.tokens import Token, TokenType\n\n# Type variable for test methods\nT = TypeVar(\"T\")\n\n\nclass TestLexer(unittest.TestCase):\n    \"\"\"Test suite for the HyperCode lexer.\"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"Create a fresh lexer instance for each test.\"\"\"\n        self.lexer = Lexer()\n\n    def test_empty_source(self) -> None:\n        \"\"\"Test that an empty source returns only an EOF token.\"\"\"\n        tokens = self.lexer.tokenize(\"\")\n        self.assertEqual(len(tokens), 1)\n        self.assertEqual(tokens[0].type, TokenType.EOF)\n\n    def test_basic_tokens(self) -> None:\n        \"\"\"Test basic token types are correctly identified.\"\"\"\n        source = \"var x = 10;\"\n        expected_types = [\n            TokenType.VAR,\n            TokenType.IDENTIFIER,\n            TokenType.EQUAL,\n            TokenType.NUMBER,\n            TokenType.SEMICOLON,\n        ]\n        tokens = self.lexer.tokenize(source)\n        self._assert_token_types(tokens, expected_types, \"Basic token test failed: \")\n\n    def test_string_literals(self):\n        \"\"\"Test string literals with various contents.\"\"\"\n        test_cases = [\n            ('\"simple\"', \"simple\"),\n            ('\"escaped \\\\n newline\"', \"escaped \\\\n newline\"),\n            ('\"unicode \"', \"unicode \"),\n        ]\n\n        for source, expected_content in test_cases:\n            with self.subTest(source=source):\n                tokens = self.lexer.tokenize(f\"var s = {source};\")\n                self.assertEqual(tokens[3].type, TokenType.STRING)\n                self.assertEqual(tokens[3].lexeme.strip('\"'), expected_content)\n\n    def test_numbers(self) -> None:\n        \"\"\"Test different number formats.\"\"\"\n        test_cases = [\n            (\"42\", 42, 42.0),\n            (\"3.14\", 3.14, 3.14),\n            (\"1e3\", 1000.0, 1000.0),\n            (\"0x2A\", 42, 42.0),  # Hex to int\n            (\"0b1010\", 10, 10.0),  # Binary to int\n        ]\n\n        for source, expected_int, expected_float in test_cases:\n            with self.subTest(source=source):\n                tokens = self.lexer.tokenize(f\"n = {source};\")\n                self.assertEqual(tokens[2].type, TokenType.NUMBER)\n\n                # Check the literal value\n                self.assertIsNotNone(tokens[2].literal)\n                self.assertEqual(tokens[2].literal, expected_int)\n\n                # For float comparison, handle potential floating point precision\n                if isinstance(tokens[2].literal, float):\n                    self.assertAlmostEqual(tokens[2].literal, expected_float, places=5)\n                else:\n                    self.assertEqual(float(tokens[2].literal), expected_float)\n\n    def test_arithmetic_expressions(self) -> None:\n        \"\"\"Test complex arithmetic expressions.\"\"\"\n        source = \"result = (x + y * 2) / (3 - 1);\"\n        expected_types = [\n            TokenType.IDENTIFIER,  # result\n            TokenType.EQUAL,  # =\n            TokenType.LPAREN,  # (\n            TokenType.IDENTIFIER,  # x\n            TokenType.PLUS,  # +\n            TokenType.IDENTIFIER,  # y\n            TokenType.STAR,  # *\n            TokenType.NUMBER,  # 2\n            TokenType.RPAREN,  # )\n            TokenType.SLASH,  # /\n            TokenType.LPAREN,  # (\n            TokenType.NUMBER,  # 3\n            TokenType.MINUS,  # -\n            TokenType.NUMBER,  # 1\n            TokenType.RPAREN,  # )\n            TokenType.SEMICOLON,  # ;\n        ]\n        tokens = self.lexer.tokenize(source)\n        self._assert_token_types(tokens, expected_types)\n\n    def test_comments(self):\n        \"\"\"Test different types of comments are properly ignored.\"\"\"\n        source = \"\"\"\n        // Single line comment\n        var x = 10;  # Another comment style\n        /* Multi-line\n           comment */\n        \"\"\"\n        tokens = self.lexer.tokenize(source)\n        # Should only find the variable declaration tokens and EOF\n        self.assertEqual(len(tokens), 6)  # var, x, =, 10, ;, EOF\n        self.assertEqual(tokens[0].type, TokenType.VAR)\n        self.assertEqual(tokens[-1].type, TokenType.EOF)\n\n    def test_error_handling(self) -> None:\n        \"\"\"Test that the lexer properly handles and reports errors.\"\"\"\n        test_cases = [\n            (\"var x = @invalid;\", \"@\", \"Invalid character\"),\n            ('var str = \"unclosed string', '\"', \"Invalid character\"),\n            (\"var num = 123abc;\", \"a\", \"Invalid character\"),\n            (\"var $invalid = 1;\", \"$\", \"Invalid character\"),\n        ]\n\n        for source, error_char, error_msg in test_cases:\n            with self.subTest(source=source):\n                # Reset lexer state\n                self.setUp()\n                tokens = self.lexer.tokenize(source)\n\n                # Should have at least one error\n                self.assertTrue(self.lexer.has_errors())\n                self.assertGreater(len(self.lexer.errors), 0)\n\n                # Check error details\n                error = self.lexer.errors[0]\n                self.assertIn(error_msg, str(error))\n                self.assertIn(error_char, str(error))\n\n                # Should still return tokens including the error token\n                error_tokens = [t for t in tokens if t.type == TokenType.UNKNOWN]\n                self.assertGreater(len(error_tokens), 0)\n                self.assertIn(error_char, error_tokens[0].lexeme)\n\n                # Should continue parsing after the error\n                self.assertEqual(tokens[-1].type, TokenType.EOF)\n\n    def test_error_recovery(self):\n        \"\"\"Test that the lexer can recover from invalid tokens and continue parsing.\"\"\"\n        source = \"\"\"\n        var x = @invalid;\n        var y = 42;  // This should still be parsed\n        print(y);    // This too\n        \"\"\"\n        tokens = self.lexer.tokenize(source)\n\n        # Should have errors\n        self.assertTrue(self.lexer.has_errors())\n\n        # Should still parse valid tokens after the error\n        var_y = next((t for t in tokens if t.lexeme == \"y\"), None)\n        print_token = next((t for t in tokens if t.lexeme == \"print\"), None)\n\n        self.assertIsNotNone(var_y)\n        self.assertIsNotNone(print_token)\n\n        # Should have proper token types\n        self.assertEqual(var_y.type, TokenType.IDENTIFIER)\n        self.assertEqual(print_token.type, TokenType.PRINT)\n\n        # Should have EOF at the end\n        self.assertEqual(tokens[-1].type, TokenType.EOF)\n\n    def _assert_token_types(\n        self, tokens: List[Token], expected_types: List[TokenType], msg: str = \"\"\n    ) -> None:\n        \"\"\"Helper to assert token types match expected types.\n\n        Args:\n            tokens: List of tokens to check\n            expected_types: List of expected token types\n            msg: Optional message to include in assertion errors\n        \"\"\"\n        self.assertEqual(\n            len(tokens),\n            len(expected_types) + 1,  # +1 for EOF\n            f\"{msg}Expected {len(expected_types) + 1} tokens (including EOF), got {len(tokens)}\",\n        )\n        for i, (token, expected_type) in enumerate(zip(tokens, expected_types)):\n            self.assertEqual(\n                token.type,\n                expected_type,\n                f\"{msg}Token {i} type mismatch: expected {expected_type}, got {token.type}\",\n            )\n\n    def test_lexer_error_class(self):\n        \"\"\"Test that LexerError is properly defined and can be instantiated.\"\"\"\n        from hypercode.core.lexer import LexerError\n\n        error = LexerError(\"Test error\", 1, 5)\n        self.assertEqual(error.message, \"Test error\")\n        self.assertEqual(error.line, 1)\n        self.assertEqual(error.column, 5)\n        self.assertIn(\"Test error\", str(error))\n        self.assertIn(\"line 1, column 5\", str(error))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
  "metadata": {},
  "relative_path": "scripts\\run_lexer.py",
  "id": "c8ee5c9bd3f9762cb888e87ff9b1b1e1"
}