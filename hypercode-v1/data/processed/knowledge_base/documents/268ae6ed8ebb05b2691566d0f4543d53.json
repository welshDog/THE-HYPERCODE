{
  "file_name": "build_knowledge_base.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\scripts\\build_knowledge_base.py",
  "file_size": 12254,
  "created": "2025-12-03T22:50:02.687823",
  "modified": "2025-12-03T22:50:26.426631",
  "file_type": "code",
  "content_hash": "3bd3a21287b413055fc5d9959c54d5fe",
  "content_type": "text",
  "content": "#!/usr/bin/env python3\n\"\"\"\nHyperCode Knowledge Base Builder\n\nThis script scans the HyperCode repository and builds a comprehensive\nknowledge base from various document types including code, markdown, PDFs, etc.\n\nUsage:\n  python build_knowledge_base.py [--rebuild] [--output-dir OUTPUT_DIR]\n\nOptions:\n  --rebuild       Rebuild the entire knowledge base from scratch\n  --output-dir    Directory to store the generated knowledge base [default: ../data/processed]\n\"\"\"\n\nimport os\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, Optional, Any\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport hashlib\nimport shutil\n\n# Add the current directory to the path so we can import document_processor\nimport sys\n\nsys.path.append(str(Path(__file__).parent.absolute()))\n\nfrom document_processor import DocumentProcessor\n\n\nclass KnowledgeBaseBuilder:\n    \"\"\"Build a knowledge base from the HyperCode repository.\"\"\"\n\n    def __init__(self, repo_root: str = \".\", output_dir: str = \"../data/processed\"):\n        \"\"\"Initialize the knowledge base builder.\n\n        Args:\n            repo_root: Root directory of the HyperCode repository\n            output_dir: Directory to store the generated knowledge base\n        \"\"\"\n        self.repo_root = Path(repo_root).resolve()\n        self.output_dir = Path(output_dir).resolve()\n        self.knowledge_base_dir = self.output_dir / \"knowledge_base\"\n        self.documents_dir = self.knowledge_base_dir / \"documents\"\n        self.index_file = self.knowledge_base_dir / \"index.json\"\n        self.metadata_file = self.knowledge_base_dir / \"metadata.json\"\n\n        # Create output directories if they don't exist\n        self.documents_dir.mkdir(parents=True, exist_ok=True)\n\n        # Initialize metadata\n        self.metadata = {\n            \"name\": \"HyperCode Knowledge Base\",\n            \"version\": \"1.0.0\",\n            \"created_at\": datetime.utcnow().isoformat(),\n            \"updated_at\": datetime.utcnow().isoformat(),\n            \"document_count\": 0,\n            \"file_types\": {},\n            \"stats\": {\n                \"code_files\": 0,\n                \"markdown_files\": 0,\n                \"pdf_files\": 0,\n                \"document_files\": 0,\n                \"other_files\": 0,\n                \"total_files\": 0,\n                \"processing_errors\": 0,\n            },\n        }\n\n        # Load existing metadata if it exists\n        if self.metadata_file.exists():\n            with open(self.metadata_file, \"r\", encoding=\"utf-8\") as f:\n                self.metadata.update(json.load(f))\n\n    def should_skip(self, path: Path) -> bool:\n        \"\"\"Check if a path should be skipped during processing.\"\"\"\n        # Skip hidden directories and files\n        if any(part.startswith(\".\") for part in path.parts):\n            return True\n\n        # Skip common directories\n        skip_dirs = {\n            \"node_modules\",\n            \".git\",\n            \"__pycache__\",\n            \".venv\",\n            \"venv\",\n            \".pytest_cache\",\n            \"dist\",\n            \"build\",\n            \".egg-info\",\n            \"coverage\",\n            \".nyc_output\",\n            \".DS_Store\",\n            \".idea\",\n            \".vscode\",\n            \"data\",\n            \"docs/_build\",\n            \"docs/_static\",\n            \"docs/_templates\",\n            \"tests\",\n        }\n\n        if path.name in skip_dirs:\n            return True\n\n        # Skip binary files and other non-text files\n        skip_extensions = {\n            \".pyc\",\n            \".pyo\",\n            \".pyd\",\n            \".so\",\n            \".dll\",\n            \".exe\",\n            \".dylib\",\n            \".class\",\n            \".jar\",\n            \".war\",\n            \".ear\",\n            \".zip\",\n            \".tar\",\n            \".gz\",\n            \".bz2\",\n            \".7z\",\n            \".rar\",\n            \".png\",\n            \".jpg\",\n            \".jpeg\",\n            \".gif\",\n            \".bmp\",\n            \".ico\",\n            \".svg\",\n            \".woff\",\n            \".woff2\",\n            \".ttf\",\n            \".eot\",\n            \".mp3\",\n            \".wav\",\n            \".ogg\",\n            \".mp4\",\n            \".avi\",\n            \".mov\",\n            \".mkv\",\n            \".pdf\",  # We'll handle PDFs separately\n            \".doc\",\n            \".docx\",\n            \".xls\",\n            \".xlsx\",\n            \".ppt\",\n            \".pptx\",  # Handled separately\n        }\n\n        if path.suffix.lower() in skip_extensions:\n            return True\n\n        return False\n\n    def get_file_type(self, path: Path) -> str:\n        \"\"\"Get the file type category.\"\"\"\n        ext = path.suffix.lower()\n\n        code_extensions = {\n            \".py\",\n            \".js\",\n            \".ts\",\n            \".jsx\",\n            \".tsx\",\n            \".java\",\n            \".c\",\n            \".cpp\",\n            \".h\",\n            \".hpp\",\n            \".cs\",\n            \".go\",\n            \".rb\",\n            \".php\",\n            \".swift\",\n            \".kt\",\n            \".scala\",\n            \".rs\",\n            \".sh\",\n            \".bash\",\n            \".zsh\",\n            \".fish\",\n            \".ps1\",\n            \".bat\",\n            \".cmd\",\n            \".m\",\n            \".r\",\n            \".lua\",\n            \".pl\",\n            \".pm\",\n            \".t\",\n            \".sql\",\n            \".html\",\n            \".css\",\n            \".scss\",\n            \".sass\",\n            \".less\",\n            \".styl\",\n            \".json\",\n            \".yaml\",\n            \".yml\",\n            \".toml\",\n            \".ini\",\n            \".cfg\",\n            \".conf\",\n            \".xml\",\n            \".md\",\n            \".markdown\",\n            \".txt\",\n        }\n\n        doc_extensions = {\n            \".pdf\",\n            \".doc\",\n            \".docx\",\n            \".xls\",\n            \".xlsx\",\n            \".ppt\",\n            \".pptx\",\n            \".odt\",\n            \".ods\",\n            \".odp\",\n            \".rtf\",\n            \".tex\",\n            \".epub\",\n            \".mobi\",\n        }\n\n        if ext in code_extensions:\n            return \"code\"\n        elif ext in doc_extensions:\n            return \"document\"\n        elif ext in {\".md\", \".markdown\"}:\n            return \"markdown\"\n        else:\n            return \"other\"\n\n    def process_file(self, file_path: Path) -> Optional[Dict[str, Any]]:\n        \"\"\"Process a single file and return its metadata.\"\"\"\n        try:\n            # Process the document based on its type\n            result = DocumentProcessor.process_document(file_path)\n\n            # Add additional metadata\n            result[\"file_type\"] = self.get_file_type(file_path)\n            result[\"relative_path\"] = str(file_path.relative_to(self.repo_root))\n\n            # Generate a unique ID for the document\n            doc_id = hashlib.md5(result[\"relative_path\"].encode(\"utf-8\")).hexdigest()\n            result[\"id\"] = doc_id\n\n            # Save the document content\n            doc_path = self.documents_dir / f\"{doc_id}.json\"\n            with open(doc_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(result, f, ensure_ascii=False, indent=2)\n\n            # Update statistics\n            self.metadata[\"stats\"][\"total_files\"] += 1\n            if result[\"file_type\"] == \"code\":\n                self.metadata[\"stats\"][\"code_files\"] += 1\n            elif result[\"file_type\"] == \"markdown\":\n                self.metadata[\"stats\"][\"markdown_files\"] += 1\n            elif result[\"file_type\"] == \"document\":\n                self.metadata[\"stats\"][\"document_files\"] += 1\n            else:\n                self.metadata[\"stats\"][\"other_files\"] += 1\n\n            # Update file type statistics\n            ext = file_path.suffix.lower()\n            self.metadata[\"file_types\"][ext] = (\n                self.metadata[\"file_types\"].get(ext, 0) + 1\n            )\n\n            return result\n\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n            self.metadata[\"stats\"][\"processing_errors\"] += 1\n            return None\n\n    def build_index(self):\n        \"\"\"Build the knowledge base index.\"\"\"\n        print(f\"üîç Building knowledge base for {self.repo_root}\")\n        print(f\"üìÅ Output directory: {self.output_dir}\")\n        print()\n\n        # Clear existing documents if rebuilding\n        if self.documents_dir.exists():\n            shutil.rmtree(self.documents_dir)\n        self.documents_dir.mkdir(parents=True, exist_ok=True)\n\n        # Reset metadata\n        self.metadata.update(\n            {\n                \"created_at\": datetime.utcnow().isoformat(),\n                \"updated_at\": datetime.utcnow().isoformat(),\n                \"document_count\": 0,\n                \"file_types\": {},\n                \"stats\": {\n                    \"code_files\": 0,\n                    \"markdown_files\": 0,\n                    \"pdf_files\": 0,\n                    \"document_files\": 0,\n                    \"other_files\": 0,\n                    \"total_files\": 0,\n                    \"processing_errors\": 0,\n                },\n            }\n        )\n\n        # Walk through the repository\n        index = []\n        file_count = 0\n\n        # Get all files first to show progress\n        all_files = []\n        for root, dirs, files in os.walk(self.repo_root):\n            # Skip ignored directories\n            dirs[:] = [d for d in dirs if not self.should_skip(Path(root) / d)]\n\n            for file in files:\n                file_path = Path(root) / file\n                if not self.should_skip(file_path):\n                    all_files.append(file_path)\n\n        # Process files with progress bar\n        for file_path in tqdm(all_files, desc=\"Processing files\"):\n            result = self.process_file(file_path)\n            if result:\n                index.append(\n                    {\n                        \"id\": result[\"id\"],\n                        \"file_name\": result[\"file_name\"],\n                        \"relative_path\": result[\"relative_path\"],\n                        \"file_type\": result[\"file_type\"],\n                        \"content_type\": result.get(\"content_type\", \"unknown\"),\n                        \"created\": result.get(\"created\", \"\"),\n                        \"modified\": result.get(\"modified\", \"\"),\n                    }\n                )\n                file_count += 1\n\n        # Update document count\n        self.metadata[\"document_count\"] = file_count\n        self.metadata[\"updated_at\"] = datetime.utcnow().isoformat()\n\n        # Save the index\n        with open(self.index_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(\n                {\"metadata\": self.metadata, \"documents\": index},\n                f,\n                ensure_ascii=False,\n                indent=2,\n            )\n\n        # Save metadata\n        with open(self.metadata_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\n\n        # Print summary\n        print(\"\\n‚úÖ Knowledge base built successfully!\")\n        print(f\"üìä Documents processed: {self.metadata['document_count']}\")\n        print(\n            f\"üìÇ File types: {', '.join(f'{k} ({v})' for k, v in self.metadata['file_types'].items())}\"\n        )\n        print(f\"üìä Stats: {json.dumps(self.metadata['stats'], indent=2)}\")\n        print(f\"üìÑ Index saved to: {self.index_file}\")\n\n\ndef main():\n    \"\"\"Main entry point for the script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Build the HyperCode knowledge base.\")\n    parser.add_argument(\n        \"--rebuild\", action=\"store_true\", help=\"Rebuild the entire knowledge base\"\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        default=\"../data/processed\",\n        help=\"Output directory for the knowledge base\",\n    )\n\n    args = parser.parse_args()\n\n    # Initialize the builder\n    builder = KnowledgeBaseBuilder(output_dir=args.output_dir)\n\n    # Build the knowledge base\n    builder.build_index()\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "metadata": {},
  "relative_path": "build_knowledge_base.py",
  "id": "268ae6ed8ebb05b2691566d0f4543d53"
}