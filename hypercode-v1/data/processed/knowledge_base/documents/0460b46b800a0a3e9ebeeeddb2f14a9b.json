{
  "file_name": "lexer.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\hypercode\\core\\lexer.py",
  "file_size": 13098,
  "created": "2025-11-23T12:01:48.553859",
  "modified": "2025-12-03T17:21:19.074324",
  "file_type": "code",
  "content_hash": "698ef86ad90561779e4b6381c3dabaff",
  "content_type": "text",
  "content": "# Copyright 2025 welshDog (Lyndz Williams)\n#\n# Licensed under the MIT License (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://opensource.org/licenses/MIT\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCore HyperCode language implementation - Lexer\n\nThis module implements the lexical analyzer for the HyperCode language.\nIt converts source code into a sequence of tokens for further processing.\n\"\"\"\n\nfrom typing import List, Optional, Any, Dict\nfrom dataclasses import dataclass\nfrom .tokens import Token, TokenType\n\n\n@dataclass\nclass LexerError(Exception):\n    \"\"\"Exception raised for errors in the lexer.\"\"\"\n\n    message: str\n    line: int\n    column: int\n\n    def __init__(self, message: str, line: int, column: int):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(f\"{message} at line {line}, column {column}\")\n\n\nclass Lexer:\n    \"\"\"Lexical analyzer for the HyperCode language.\"\"\"\n\n    def __init__(self, source: str = \"\"):\n        \"\"\"Initialize the lexer with source code.\n\n        Args:\n            source: The source code to tokenize\n        \"\"\"\n        self.source = source\n        self.tokens: List[Token] = []\n        self.errors: List[LexerError] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n\n    def tokenize(self, source: Optional[str] = None) -> List[Token]:\n        \"\"\"Convert source code into a list of tokens.\n\n        Args:\n            source: Optional source code to tokenize. If not provided,\n                   uses the source passed to the constructor.\n\n        Returns:\n            List of Token objects, always ending with an EOF token\n        \"\"\"\n        if source is not None:\n            self.source = source\n\n        self.tokens = []\n        self.errors = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n\n        while not self.is_at_end():\n            self.start = self.current\n            self.scan_token()\n\n        # Add EOF token\n        self.add_token(TokenType.EOF, \"\")\n        return self.tokens\n\n    # Add other Lexer methods here (scan_token, add_token, etc.)\n\n    def is_at_end(self) -> bool:\n        \"\"\"Check if we've consumed all characters.\"\"\"\n        return self.current >= len(self.source)\n\n\nif __name__ == \"__main__\":\n    # Test code\n    lexer = Lexer(\"main { }\")\n    tokens = lexer.tokenize()\n    for token in tokens:\n        print(token)\n\n\nclass LexerError(Exception):\n    \"\"\"Exception raised for errors in the lexer.\"\"\"\n\n    def __init__(self, message: str, line: int, column: int):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(f\"{message} at line {line}, column {column}\")\n\n\nclass Lexer:\n    \"\"\"\n    Lexical analyzer for the HyperCode language.\n\n    Converts source code into a sequence of tokens that can be processed\n    by the parser.\n    \"\"\"\n\n    # We'll handle tokens directly in scan_token() instead of using a TOKENS list\n\n    def __init__(self, source: str = \"\"):\n        \"\"\"Initialize the lexer with source code.\n\n        Args:\n            source: The source code to tokenize\n        \"\"\"\n        self.source = source\n        self.tokens: List[Token] = []\n        self.errors: List[LexerError] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n\n    def tokenize(self, source: Optional[str] = None) -> List[Token]:\n        \"\"\"Convert source code into a list of tokens.\n\n        Args:\n            source: Optional source code to tokenize. If not provided,\n                   uses the source passed to the constructor.\n\n        Returns:\n            List of Token objects, always ending with an EOF token\n        \"\"\"\n        if source is not None:\n            self.source = source\n\n        self.tokens = []\n        self.errors = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n\n        while not self.is_at_end():\n            self.start = self.current\n            self.scan_token()\n\n        # Add EOF token\n        self.add_token(TokenType.EOF, \"\")\n        return self.tokens\n\n    def scan_token(self) -> None:\n        \"\"\"Scan the next token from the source.\"\"\"\n        # Skip whitespace\n        while not self.is_at_end() and self.peek() in \" \\t\\r\":\n            self.advance()\n\n        if self.is_at_end():\n            return\n\n        char = self.advance()\n        self.start = self.current - 1  # Set start to the beginning of the token\n\n        # Single-character tokens\n        if char == \"(\":\n            self.add_token(TokenType.LPAREN)\n        elif char == \")\":\n            self.add_token(TokenType.RPAREN)\n        elif char == \"{\":\n            self.add_token(TokenType.LBRACE)\n        elif char == \"}\":\n            self.add_token(TokenType.RBRACE)\n        elif char == \"[\":\n            # Check for [ow:label] syntax\n            if (\n                self.peek() == \"o\"\n                and self.peek_next() == \"w\"\n                and self.source[self.current] == \":\"\n            ):\n                # Skip 'ow:' part\n                while self.peek() != \"]\" and not self.is_at_end():\n                    self.advance()\n                if not self.is_at_end() and self.peek() == \"]\":\n                    self.advance()  # Consume ']'\n                    self.add_token(\n                        TokenType.IDENTIFIER,\n                        self.source[self.start + 1 : self.current - 1],\n                    )\n            else:\n                self.add_token(TokenType.LBRACKET)\n        elif char == \"]\":\n            self.add_token(TokenType.RBRACKET)\n        elif char == \",\":\n            self.add_token(TokenType.COMMA)\n        elif char == \".\":\n            self.add_token(TokenType.DOT)\n        elif char == \"-\":\n            self.add_token(TokenType.MINUS)\n        elif char == \"+\":\n            self.add_token(TokenType.PLUS)\n        elif char == \";\":\n            self.add_token(TokenType.SEMICOLON)\n        elif char == \"*\":\n            self.add_token(TokenType.STAR)\n        elif char == \":\":\n            self.add_token(TokenType.COLON)\n        elif char == \"!\":\n            self.add_token(TokenType.BANG_EQUAL if self.match(\"=\") else TokenType.BANG)\n        elif char == \"=\":\n            self.add_token(\n                TokenType.EQUAL_EQUAL if self.match(\"=\") else TokenType.EQUAL\n            )\n        elif char == \"<\":\n            self.add_token(TokenType.LESS_EQUAL if self.match(\"=\") else TokenType.LESS)\n        elif char == \">\":\n            self.add_token(\n                TokenType.GREATER_EQUAL if self.match(\"=\") else TokenType.GREATER\n            )\n        elif char == '\"' or char == \"'\":\n            self.string(char)\n        elif char.isdigit():\n            self.number()\n        elif char.isalpha() or char == \"_\":\n            self.identifier()\n        elif char == \"/\":\n            if self.match(\"/\"):\n                # Skip comments\n                while self.peek() != \"\\n\" and not self.is_at_end():\n                    self.advance()\n            else:\n                self.add_token(TokenType.SLASH)\n        elif char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            # Skip unknown characters for now\n            pass\n\n    def string(self, quote: str) -> None:\n        \"\"\"Scan a string literal.\"\"\"\n        start_line = self.line\n        start_col = self.column\n        value = \"\"\n\n        while self.peek() != quote and not self.is_at_end():\n            if self.peek() == \"\\n\":\n                self.line += 1\n                self.column = 0\n            value += self.advance()\n\n        if self.is_at_end():\n            self.error(\"Unterminated string\", start_line, start_col)\n            return\n\n        # The closing quote\n        self.advance()\n\n        # Add the string token without the surrounding quotes\n        self.add_token(TokenType.STRING, value[1:-1])\n\n    def number(self) -> None:\n        \"\"\"Scan a number literal.\"\"\"\n        while self.peek().isdigit():\n            self.advance()\n\n        # Look for a fractional part\n        if self.peek() == \".\" and self.peek_next().isdigit():\n            # Consume the \".\"\n            self.advance()\n\n            while self.peek().isdigit():\n                self.advance()\n\n        # Look for exponent\n        if self.peek().lower() == \"e\":\n            self.advance()  # Consume 'e' or 'E'\n            if self.peek() in \"+-\":\n                self.advance()  # Consume sign\n\n            if not self.peek().isdigit():\n                self.error(\"Expected digit after exponent\")\n                return\n\n            while self.peek().isdigit():\n                self.advance()\n\n        # Convert to int or float\n        num_str = self.source[self.start : self.current]\n        if \".\" in num_str or \"e\" in num_str.lower():\n            self.add_token(TokenType.NUMBER, float(num_str))\n        else:\n            self.add_token(TokenType.NUMBER, int(num_str))\n\n    def identifier(self) -> None:\n        \"\"\"Scan an identifier or keyword.\"\"\"\n        while not self.is_at_end() and (self.peek().isalnum() or self.peek() == \"_\"):\n            self.advance()\n\n        # Check if the identifier is a reserved keyword\n        text = self.source[self.start : self.current]\n        token_type = self.KEYWORDS.get(text.lower(), TokenType.IDENTIFIER)\n        self.add_token(token_type)\n\n    def match(self, expected: str) -> bool:\n        \"\"\"Match the next character if it matches the expected character.\"\"\"\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n\n        self.current += 1\n        self.column += 1\n        return True\n\n    def peek(self) -> str:\n        \"\"\"Look at the next character without consuming it.\"\"\"\n        if self.is_at_end():\n            return \"\\0\"\n        return self.source[self.current]\n\n    def peek_next(self) -> str:\n        \"\"\"Look at the character after the next one without consuming it.\"\"\"\n        if self.current + 1 >= len(self.source):\n            return \"\\0\"\n        return self.source[self.current + 1]\n\n    def advance(self) -> str:\n        \"\"\"Consume and return the next character.\"\"\"\n        if self.is_at_end():\n            return \"\\0\"\n\n        char = self.source[self.current]\n        self.current += 1\n        self.column += 1\n        return char\n\n    def add_token(self, token_type: TokenType, literal: Any = None) -> None:\n        \"\"\"Add a new token to the tokens list.\"\"\"\n        text = self.source[self.start : self.current]\n        if literal is None:\n            literal = text.strip()\n        self.tokens.append(Token(token_type, text, literal, self.line, self.column))\n        self.column += len(text)\n\n    def error(\n        self, message: str, line: Optional[int] = None, column: Optional[int] = None\n    ) -> None:\n        \"\"\"Record a lexer error.\n\n        Args:\n            message: The error message\n            line: The line number where the error occurred (defaults to current line)\n            column: The column number where the error occurred (defaults to current column)\n        \"\"\"\n        error_line = line if line is not None else self.line\n        error_col = column if column is not None else self.column\n        self.errors.append(LexerError(message, error_line, error_col))\n\n    def is_at_end(self) -> bool:\n        \"\"\"Check if we've consumed all characters.\"\"\"\n        return self.current >= len(self.source)\n\n    KEYWORDS = {\n        # Standard keywords\n        \"if\": TokenType.IF,\n        \"else\": TokenType.ELSE,\n        \"for\": TokenType.FOR,\n        \"while\": TokenType.WHILE,\n        \"fun\": TokenType.FUN,\n        \"function\": TokenType.FUNCTION,\n        \"return\": TokenType.RETURN,\n        \"let\": TokenType.LET,\n        \"const\": TokenType.CONST,\n        \"var\": TokenType.VAR,\n        \"print\": TokenType.PRINT,\n        \"true\": TokenType.TRUE,\n        \"false\": TokenType.FALSE,\n        \"nil\": TokenType.NIL,\n        \"class\": TokenType.CLASS,\n        # HyperCode specific keywords\n        \"main\": TokenType.MAIN,\n        \"emit\": TokenType.EMIT,\n        \"output\": TokenType.OUTPUT,\n        \"input\": TokenType.INPUT,\n        \"store\": TokenType.STORE,\n        \"goto\": TokenType.GOTO,\n        \"label\": TokenType.LABEL,\n        \"repeat\": TokenType.REPEAT,\n        \"times\": TokenType.TIMES,\n    }\n",
  "metadata": {},
  "relative_path": "src\\hypercode\\core\\lexer.py",
  "id": "0460b46b800a0a3e9ebeeeddb2f14a9b"
}