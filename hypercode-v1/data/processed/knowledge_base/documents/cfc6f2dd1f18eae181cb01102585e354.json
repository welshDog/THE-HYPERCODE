{
  "file_name": "knowledge_base.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\src\\hypercode\\knowledge_base.py",
  "file_size": 18558,
  "created": "2025-11-18T01:05:06.161211",
  "modified": "2025-11-26T02:00:12.353618",
  "file_type": "code",
  "content_hash": "98d6e537f34a70af97b748140af64e4e",
  "content_type": "text",
  "content": "#!/usr/bin/env python3\n\"\"\"\nHyperCode Knowledge Base - Perplexity Space Integration\nManages research data from Perplexity Space for API context\n\"\"\"\n\nimport hashlib\nimport json\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass ResearchDocument:\n    \"\"\"Represents a research document from Perplexity Space\"\"\"\n\n    id: str\n    title: str\n    content: str\n    url: Optional[str] = None\n    tags: List[str] = None\n    created_at: str = None\n    last_updated: str = None\n\n    def __post_init__(self):\n        if self.tags is None:\n            self.tags = []\n        if self.created_at is None:\n            self.created_at = datetime.now().isoformat()\n        if self.last_updated is None:\n            self.last_updated = datetime.now().isoformat()\n\n    def generate_id(self) -> str:\n        \"\"\"Generate unique ID from content hash\"\"\"\n        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:8]\n        return f\"doc_{content_hash}\"\n\n    def validate(self) -> bool:\n        \"\"\"Validate document data\"\"\"\n        if not self.title or len(self.title.strip()) == 0:\n            raise ValueError(\"Document title cannot be empty\")\n        if not self.content or len(self.content.strip()) == 0:\n            raise ValueError(\"Document content cannot be empty\")\n        if len(self.title) > 1000:\n            raise ValueError(\"Title too long (max 1000 characters)\")\n        if len(self.content) > 1000000:\n            raise ValueError(\"Content too long (max 1MB)\")\n        return True\n\n    def update_timestamp(self):\n        \"\"\"Update the last_updated timestamp\"\"\"\n        self.last_updated = datetime.now().isoformat()\n        if hasattr(self, \"version\"):\n            self.version += 1\n\n\n# Related terms dictionary for search expansion\nRELATED_TERMS = {\n    \"pillar\": [\"pillars\", \"column\", \"foundation\", \"core\"],\n    \"pillars\": [\"pillar\", \"column\", \"foundation\", \"core\"],\n    \"audit\": [\"auditing\", \"checklist\", \"review\", \"assessment\"],\n    \"auditing\": [\"audit\", \"checklist\", \"review\", \"assessment\"],\n    \"phase\": [\"phases\", \"stage\", \"step\", \"milestone\"],\n    \"phases\": [\"phase\", \"stage\", \"step\", \"milestone\"],\n    \"neurodiversity\": [\"neurodivergent\", \"adhd\", \"autism\", \"dyslexia\"],\n    \"neurodivergent\": [\"neurodiversity\", \"adhd\", \"autism\", \"dyslexia\"],\n    \"implementation\": [\"implement\", \"deploy\", \"execute\", \"build\"],\n    \"implement\": [\"implementation\", \"deploy\", \"execute\", \"build\"],\n    \"metadata\": [\"space\", \"author\", \"creator\", \"project\"],\n    \"space\": [\"metadata\", \"author\", \"creator\", \"project\"],\n    \"philosophy\": [\"principles\", \"ideas\", \"concepts\", \"beliefs\"],\n    \"principles\": [\"philosophy\", \"ideas\", \"concepts\", \"beliefs\"],\n    \"future\": [\"quantum\", \"dna\", \"ai\", \"technologies\"],\n    \"quantum\": [\"future\", \"dna\", \"ai\", \"technologies\"],\n    \"dna\": [\"future\", \"quantum\", \"ai\", \"technologies\"],\n    \"ai\": [\"future\", \"quantum\", \"dna\", \"technologies\"],\n    \"technologies\": [\"future\", \"quantum\", \"dna\", \"ai\"],\n    \"message\": [\"core\", \"mission\", \"vision\", \"statement\"],\n    \"core\": [\"message\", \"mission\", \"vision\", \"statement\"],\n    \"technical\": [\"features\", \"specifications\", \"architecture\"],\n    \"features\": [\"technical\", \"specifications\", \"architecture\"],\n    \"research\": [\"methodology\", \"paper\", \"living\", \"study\"],\n    \"methodology\": [\"research\", \"paper\", \"living\", \"study\"],\n    \"community\": [\"collaboration\", \"open source\", \"contribution\"],\n    \"collaboration\": [\"community\", \"open source\", \"contribution\"],\n    \"roadmap\": [\"phases\", \"timeline\", \"milestones\", \"implementation\"],\n    \"timeline\": [\"roadmap\", \"phases\", \"milestones\", \"implementation\"],\n    \"impact\": [\"vision\", \"societal\", \"innovation\", \"long-term\"],\n    \"vision\": [\"impact\", \"societal\", \"innovation\", \"long-term\"],\n    \"resources\": [\"references\", \"historical\", \"languages\", \"brainfuck\"],\n    \"references\": [\"resources\", \"historical\", \"languages\", \"brainfuck\"],\n    \"action\": [\"call\", \"join\", \"contribute\", \"movement\"],\n    \"call\": [\"action\", \"join\", \"contribute\", \"movement\"],\n}\n\n\nclass HyperCodeKnowledgeBase:\n    \"\"\"Knowledge base for HyperCode research data\"\"\"\n\n    def __init__(self, kb_path: str = \"data/hypercode_knowledge_base.json\"):\n        self.kb_path = Path(kb_path)\n        self.kb_path.parent.mkdir(exist_ok=True)\n        self.documents: Dict[str, ResearchDocument] = {}\n        self.load()\n\n    def load(self):\n        \"\"\"Load knowledge base from file\"\"\"\n        if self.kb_path.exists():\n            try:\n                with open(self.kb_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                    for doc_id, doc_data in data.items():\n                        self.documents[doc_id] = ResearchDocument(**doc_data)\n                print(f\"Loaded {len(self.documents)} documents from knowledge base\")\n            except Exception as e:\n                print(f\"Error loading knowledge base: {str(e)}\")\n                self.documents = {}\n        else:\n            print(\"Creating new knowledge base\")\n            self.documents = {}\n\n    def save(self):\n        \"\"\"Save knowledge base to file\"\"\"\n        try:\n            data = {doc_id: asdict(doc) for doc_id, doc in self.documents.items()}\n            with open(self.kb_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            print(f\"Saved {len(self.documents)} documents to knowledge base\")\n        except Exception as e:\n            print(f\"Error saving knowledge base: {str(e)}\")\n\n    def add_document(\n        self,\n        title: str,\n        content: str,\n        url: Optional[str] = None,\n        tags: List[str] = None,\n    ) -> str:\n        \"\"\"Add a new research document\"\"\"\n        doc = ResearchDocument(\n            id=\"\",  # Will be generated\n            title=title,\n            content=content,\n            url=url,\n            tags=tags or [],\n        )\n        doc.id = doc.generate_id()\n\n        # Check if document already exists\n        if doc.id in self.documents:\n            print(f\"Updating existing document: {doc.title}\")\n            self.documents[doc.id].last_updated = datetime.now().isoformat()\n        else:\n            print(f\"Adding new document: {doc.title}\")\n\n        self.documents[doc.id] = doc\n        self.save()\n        return doc.id\n\n    def search_documents(self, query: str, limit: int = 5) -> List[ResearchDocument]:\n        \"\"\"Search documents by query\"\"\"\n        query_lower = query.lower().strip()\n\n        # Handle empty queries\n        if not query_lower:\n            return []\n\n        results = []\n\n        for doc in self.documents.values():\n            score = 0\n\n            # Title matches (highest weight)\n            if query_lower in doc.title.lower():\n                score += 10\n\n            # Content matches (count occurrences)\n            content_lower = doc.content.lower()\n            query_words = query_lower.split()\n            for word in query_words:\n                count = content_lower.count(word)\n                if count > 0:\n                    score += count\n\n            # Tag matches\n            if doc.tags:\n                tags_lower = \" \".join(doc.tags).lower()\n                if query_lower in tags_lower:\n                    score += 5\n\n            # Partial word matching for better coverage\n            query_words = query_lower.split()\n            for word in query_words:\n                if len(word) > 3:  # Only match longer words\n                    if word in content_lower:\n                        score += 1\n                    if word in doc.title.lower():\n                        score += 3\n\n            # Related term matching (only if query is exact match to a key)\n            if query_lower in RELATED_TERMS:\n                for related in RELATED_TERMS[query_lower]:\n                    if related in content_lower:\n                        score += 2\n                    if related in doc.title.lower():\n                        score += 4\n\n            # Boost space data for general queries (but not for specific terms)\n            if (\n                \"space-data\" in doc.tags\n                and len(query_words) == 1\n                and query_words[0] in [\"hypercode\", \"what\", \"tell\", \"explain\", \"about\"]\n            ):\n                score += 3\n\n            if score > 0:\n                results.append((doc, score))\n\n        # Sort by score and return top results\n        results.sort(key=lambda x: x[1], reverse=True)\n        return [doc for doc, score in results[:limit]]\n\n    def get_context_for_query(self, query: str, max_context_length: int = 4000) -> str:\n        \"\"\"Get relevant context for a query\"\"\"\n        relevant_docs = self.search_documents(query, limit=3)\n\n        if not relevant_docs:\n            return \"No relevant research data found.\"\n\n        context_parts = [\"Relevant HyperCode Research Data:\\n\"]\n\n        for doc in relevant_docs:\n            context_parts.append(f\"## {doc.title}\")\n\n            # Truncate content if too long\n            content = doc.content\n            if len(content) > 1000:\n                content = content[:1000] + \"...\"\n\n            context_parts.append(content)\n            if doc.url:\n                context_parts.append(f\"Source: {doc.url}\")\n            context_parts.append(\"\")  # Empty line\n\n        # Join and truncate if necessary\n        full_context = \"\\n\".join(context_parts)\n\n        if len(full_context) > max_context_length:\n            full_context = full_context[:max_context_length] + \"...\"\n\n        return full_context\n\n    def list_documents(self) -> List[ResearchDocument]:\n        \"\"\"List all documents\"\"\"\n        return list(self.documents.values())\n\n    def get_document(self, doc_id: str) -> Optional[ResearchDocument]:\n        \"\"\"Get a specific document by ID\"\"\"\n        return self.documents.get(doc_id)\n\n    def delete_document(self, doc_id: str) -> bool:\n        \"\"\"Delete a document\"\"\"\n        if doc_id in self.documents:\n            del self.documents[doc_id]\n            self.save()\n            return True\n        return False\n\n    def update_document(self, doc_id: str, **kwargs) -> bool:\n        \"\"\"Update an existing document\"\"\"\n        if doc_id not in self.documents:\n            return False\n\n        doc = self.documents[doc_id]\n        for key, value in kwargs.items():\n            if hasattr(doc, key):\n                setattr(doc, key, value)\n\n        doc.update_timestamp()\n        self.save()\n        return True\n\n    def search_by_tags(\n        self, tags: List[str], operator: str = \"AND\"\n    ) -> List[ResearchDocument]:\n        \"\"\"Search documents by tags with AND/OR operators\"\"\"\n        results = []\n\n        for doc in self.documents.values():\n            doc_tags = set(doc.tags)\n            search_tags = set(tags)\n\n            if operator == \"AND\":\n                if search_tags.issubset(doc_tags):\n                    results.append(doc)\n            elif operator == \"OR\":\n                if doc_tags.intersection(search_tags):\n                    results.append(doc)\n\n        return results\n\n    def get_document_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about the knowledge base\"\"\"\n        total_docs = len(self.documents)\n        tag_counts = defaultdict(int)\n        total_content_length = 0\n\n        for doc in self.documents.values():\n            for tag in doc.tags:\n                tag_counts[tag] += 1\n            total_content_length += len(doc.content)\n\n        return {\n            \"total_documents\": total_docs,\n            \"total_content_length\": total_content_length,\n            \"average_content_length\": total_content_length / max(total_docs, 1),\n            \"unique_tags\": len(tag_counts),\n            \"tag_distribution\": dict(tag_counts),\n            \"oldest_document\": min(\n                (doc.created_at for doc in self.documents.values()), default=None\n            ),\n            \"newest_document\": max(\n                (doc.created_at for doc in self.documents.values()), default=None\n            ),\n        }\n\n    def export_format(self, format_type: str = \"json\") -> str:\n        \"\"\"Export knowledge base in different formats\"\"\"\n        if format_type == \"json\":\n            return json.dumps(\n                {doc_id: asdict(doc) for doc_id, doc in self.documents.items()},\n                indent=2,\n            )\n        elif format_type == \"markdown\":\n            lines = [\"# HyperCode Knowledge Base Export\\n\"]\n            for doc in self.documents.values():\n                lines.append(f\"## {doc.title}\")\n                lines.append(f\"**Tags:** {', '.join(doc.tags)}\")\n                lines.append(f\"**Created:** {doc.created_at}\")\n                if doc.url:\n                    lines.append(f\"**URL:** {doc.url}\")\n                lines.append(\"\")\n                lines.append(doc.content)\n                lines.append(\"\\n---\\n\")\n            return \"\\n\".join(lines)\n        else:\n            raise ValueError(f\"Unsupported format: {format_type}\")\n\n    def validate_all_documents(self) -> List[str]:\n        \"\"\"Validate all documents and return list of errors\"\"\"\n        errors = []\n        for doc_id, doc in self.documents.items():\n            try:\n                doc.validate()\n            except ValueError as e:\n                errors.append(f\"Document {doc_id} ({doc.title}): {str(e)}\")\n        return errors\n\n    def cleanup_duplicates(self) -> int:\n        \"\"\"Remove duplicate documents based on content hash\"\"\"\n        seen_hashes = set()\n        duplicates = []\n\n        for doc_id, doc in self.documents.items():\n            content_hash = hashlib.md5(doc.content.encode()).hexdigest()\n            if content_hash in seen_hashes:\n                duplicates.append(doc_id)\n            else:\n                seen_hashes.add(content_hash)\n\n        for doc_id in duplicates:\n            del self.documents[doc_id]\n\n        if duplicates:\n            self.save()\n\n        return len(duplicates)\n\n\ndef initialize_sample_data():\n    \"\"\"Initialize with sample HyperCode research data\"\"\"\n    kb = HyperCodeKnowledgeBase()\n\n    # Sample documents based on what would likely be in your Perplexity Space\n    sample_docs = [\n        {\n            \"title\": \"HyperCode Language Specification v1.0.0\",\n            \"content\": \"\"\"\n            HyperCode is a neurodivergent-first programming language that supports:\n            - Multiple syntax modes (visual, text, spatial)\n            - AI-assisted code generation and debugging\n            - Multi-backend compilation (quantum, DNA, traditional)\n            - Built-in accessibility features\n\n            Core syntax elements include:\n            - Declarative statements with visual markers\n            - Pattern matching for cognitive flexibility\n            - Spatial code execution (2D and 3D)\n            - Natural language programming interfaces\n            \"\"\",\n            \"tags\": [\"specification\", \"language\", \"syntax\"],\n        },\n        {\n            \"title\": \"Neurodivergent-First Design Principles\",\n            \"content\": \"\"\"\n            Design principles for neurodivergent developers:\n\n            1. Cognitive Flexibility:\n            - Multiple ways to express the same concept\n            - No single \"right\" way to write code\n            - Support for different thinking patterns\n\n            2. Sensory Accommodation:\n            - Customizable visual themes\n            - Adjustable information density\n            - Optional sound/haptic feedback\n\n            3. Executive Function Support:\n            - Built-in reminders and planning tools\n            - Automatic code organization\n            - Progress tracking and milestones\n\n            4. Communication Clarity:\n            - Explicit, literal error messages\n            - No ambiguous metaphors or idioms\n            - Clear, structured documentation\n            \"\"\",\n            \"tags\": [\"design\", \"accessibility\", \"neurodiversity\"],\n        },\n        {\n            \"title\": \"Multi-Backend Architecture\",\n            \"content\": \"\"\"\n            HyperCode's compilation architecture supports:\n\n            Traditional Backends:\n            - x86/ARM assembly\n            - WebAssembly\n            - LLVM IR\n\n            AI Backends:\n            - GPT-4/ Claude integration\n            - Local model execution\n            - Hybrid AI-human compilation\n\n            Exotic Backends:\n            - Quantum computing circuits\n            - DNA strand displacement\n            - Neural network processors\n\n            Backend Selection:\n            - Automatic based on problem type\n            - Manual override available\n            - Multi-backend optimization\n            \"\"\",\n            \"tags\": [\"architecture\", \"compilation\", \"backends\"],\n        },\n        {\n            \"title\": \"Spatial Programming Paradigm\",\n            \"content\": \"\"\"\n            2D Spatial Code Execution:\n\n            Benefits:\n            - Visual representation of program flow\n            - Intuitive for visual thinkers\n            - Natural for diagram-based reasoning\n\n            Implementation:\n            - Code execution follows directional arrows\n            - Supports wrapping (Befunge-style)\n            - Multiple instruction streams\n\n            Use Cases:\n            - Data flow visualization\n            - Algorithm animation\n            - Educational visualization\n            \"\"\",\n            \"tags\": [\"spatial\", \"2d\", \"visual\"],\n        },\n    ]\n\n    for doc_data in sample_docs:\n        kb.add_document(**doc_data)\n\n    print(f\"‚úÖ Initialized knowledge base with {len(sample_docs)} sample documents\")\n    return kb\n\n\nif __name__ == \"__main__\":\n    # Initialize sample data\n    kb = initialize_sample_data()\n\n    # Test search functionality\n    print(\"\\nüîç Testing search functionality:\")\n    test_queries = [\"neurodivergent\", \"compilation\", \"spatial\", \"AI\"]\n\n    for query in test_queries:\n        results = kb.search_documents(query)\n        print(f\"\\nQuery: '{query}' - Found {len(results)} results\")\n        for doc in results:\n            print(f\"  - {doc.title}\")\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\src\\hypercode\\knowledge_base.py",
  "id": "cfc6f2dd1f18eae181cb01102585e354"
}