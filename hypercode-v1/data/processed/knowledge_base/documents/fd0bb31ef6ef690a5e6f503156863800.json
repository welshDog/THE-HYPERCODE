{
  "file_name": "lexer.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\lexer.py",
  "file_size": 11448,
  "created": "2025-11-23T12:01:48.542065",
  "modified": "2025-11-30T19:02:16.179668",
  "file_type": "code",
  "content_hash": "d2537ce83e7584141c17c02c5c43ae92",
  "content_type": "text",
  "content": "# src/core/lexer.py\n\"\"\"\nHyperCode Lexer Module\n\nTokenizes HyperCode source code into a stream of tokens for parsing.\nHandles keywords, identifiers, literals, operators, and special syntax.\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom .tokens import Token, TokenType, KEYWORDS\n\n\n@dataclass\nclass LexerError:\n    \"\"\"\n    Represents a lexical analysis error.\n\n    Attributes:\n        message: Description of the error\n        line: Line number where the error occurred (1-based)\n        column: Column number where the error occurred (1-based)\n        length: Length of the problematic token (default: 1)\n    \"\"\"\n    message: str\n    line: int\n    column: int\n    length: int = 1\n\n\nclass Lexer:\n    \"\"\"\n    Lexical analyzer for the HyperCode programming language.\n\n    Converts source code into a sequence of tokens that can be parsed.\n    Handles syntax highlighting, error reporting, and source mapping.\n    \"\"\"\n\n    def __init__(self, source: str):\n        \"\"\"Initialize the lexer with source code.\"\"\"\n        self.source = source\n        self.tokens: List[Token] = []\n        self.errors: List[LexerError] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n        \n        # Pre-compile regex patterns for better performance\n        self.identifier_regex = re.compile(r'[a-zA-Z_][a-zA-Z0-9_]*')\n        self.number_regex = re.compile(r'\\d+')\n        \n        # Use a lookup table for single-character tokens\n        self.single_char_tokens = {\n            '(': TokenType.LEFT_PAREN,\n            ')': TokenType.RIGHT_PAREN,\n            '{': TokenType.LEFT_BRACE,\n            '}': TokenType.RIGHT_BRACE,\n            ',': TokenType.COMMA,\n            '.': TokenType.DOT,\n            '+': TokenType.PLUS,\n            ';': TokenType.SEMICOLON,\n            '*': TokenType.STAR,\n            '%': TokenType.PERCENT,\n            '!': TokenType.BANG,\n            '=': TokenType.EQUAL,\n            '<': TokenType.LESS,\n            '>': TokenType.GREATER,\n            ':': TokenType.COLON,\n            '?': TokenType.QUESTION,\n            '|': TokenType.PIPE,\n            '@': TokenType.AT,\n        }\n\n    def scan_tokens(self) -> List[Token]:\n        \"\"\"Scan the source code and return a list of tokens.\"\"\"\n        while not self.is_at_end():\n            self.start = self.current\n            self.scan_token()\n        \n        # Add EOF token\n        self.add_token(TokenType.EOF)\n        return self.tokens\n\n    def scan_token(self):\n        \"\"\"Scan a single token.\"\"\"\n        c = self.advance()\n        \n        # Handle single-character tokens first\n        if c in self.single_char_tokens:\n            self.add_token(self.single_char_tokens[c])\n            return\n\n        # Handle multi-character tokens\n        if c == '-':\n            if self.match('>'):\n                self.add_token(TokenType.ARROW)\n            else:\n                self.add_token(TokenType.MINUS)\n        elif c == '=':\n            if self.match('='):\n                self.add_token(TokenType.EQUAL_EQUAL)\n            else:\n                self.add_token(TokenType.EQUAL)\n        elif c == '!':\n            if self.match('='):\n                self.add_token(TokenType.BANG_EQUAL)\n            else:\n                self.add_token(TokenType.BANG)\n        elif c == '<':\n            if self.match('='):\n                self.add_token(TokenType.LESS_EQUAL)\n            else:\n                self.add_token(TokenType.LESS)\n        elif c == '>':\n            if self.match('='):\n                self.add_token(TokenType.GREATER_EQUAL)\n            else:\n                self.add_token(TokenType.GREATER)\n        \n        # Handle comments and division\n        elif c == '/':\n            if self.peek() == '/':\n                # A comment goes until the end of the line.\n                while self.peek() != '\\n' and not self.is_at_end():\n                    self.advance()\n            elif self.peek() == '*' and self.peek_next() == '*':\n                self.docstring()\n            else:\n                self.add_token(TokenType.SLASH)\n\n        # Handle numbers\n        elif c.isdigit():\n            self.number()\n            \n        # Handle strings\n        elif c == '\"':\n            self.string()\n            \n        # Handle string interpolation (f-strings)\n        elif c == 'f' and self.peek() == '\"':\n            self.advance()  # Consume the 'f'\n            self.string(interpolated=True)\n            \n        # Handle whitespace\n        elif c in ' \\r\\t':\n            pass\n            \n        # Handle newlines\n        elif c == '\\n':\n            self.line += 1\n            self.column = 1\n            \n        # Handle identifiers and keywords\n        elif c.isalpha() or c == '_':\n            self.identifier()\n            \n        # Handle errors\n        else:\n            self.error(f\"Unexpected character: {c}\")\n\n    def number(self):\n        \"\"\"Lex a number literal.\"\"\"\n        while self.peek().isdigit() or self.peek() == '_':\n            self.advance()\n\n        # Look for decimal part\n        if self.peek() == '.' and (self.peek_next().isdigit() or self.peek_next() == '_'):\n            self.advance()  # Consume the '.'\n\n            while self.peek().isdigit() or self.peek() == '_':\n                self.advance()\n\n        # Look for scientific notation\n        if self.peek().lower() == 'e':\n            self.advance()  # Consume 'e' or 'E'\n            if self.peek() in '+-':\n                self.advance()  # Consume sign\n            while self.peek().isdigit() or self.peek() == '_':\n                self.advance()\n\n        # Parse the number\n        value = self.source[self.start:self.current]\n        # Handle underscores in numbers (e.g., 1_000_000)\n        value = value.replace('_', '')\n        try:\n            if '.' in value or 'e' in value.lower():\n                self.add_token(TokenType.NUMBER, float(value))\n            else:\n                # Try to parse as int first, fall back to float if too large\n                self.add_token(TokenType.NUMBER, int(value))\n        except ValueError:\n            self.error(f\"Invalid number: {value}\")\n\n    def string(self, interpolated=False):\n        \"\"\"Lex a string literal.\"\"\"\n        start_line = self.line\n        start_col = self.column\n        \n        while (not self.is_at_end()) and (self.peek() != '\"' or (interpolated and self.peek() == '{')):\n            if self.peek() == '\\n':\n                self.line += 1\n                self.column = 0\n            elif self.peek() == '\\\\':\n                # Handle escape sequences\n                self.advance()\n                if self.is_at_end():\n                    break\n                # Handle common escape sequences\n                if self.peek() in '\\\\\"nrt':\n                    self.advance()\n            self.advance()\n            \n            # Handle string interpolation\n            if interpolated and self.peek() == '{':\n                # Add the string part before the interpolation\n                value = self.source[self.start + 1:self.current]  # +1 to exclude the opening quote\n                self.add_token(TokenType.STRING, value)\n                \n                # Add the interpolation start token\n                self.advance()  # Consume the {\n                self.add_token(TokenType.LEFT_BRACE)\n                \n                # Reset to start a new token after the {\n                self.start = self.current\n                return\n                \n        # Unterminated string\n        if self.is_at_end():\n            self.error(\"Unterminated string\", start_line, start_col)\n            return\n            \n        # The closing \"\n        self.advance()\n        \n        # Get the string value (without quotes)\n        value = self.source[self.start + 1:self.current - 1]\n        self.add_token(TokenType.STRING, value)\n\n    def docstring(self):\n        \"\"\"Lex a docstring.\"\"\"\n        start_line = self.line\n        start_col = self.column\n        \n        # Skip the opening /**\n        self.advance()  # Skip *\n        self.advance()  # Skip *\n        \n        while not (self.peek() == '*' and self.peek_next() == '/'):\n            if self.is_at_end():\n                self.error(\"Unterminated docstring\", start_line, start_col)\n                return\n            if self.peek() == '\\n':\n                self.line += 1\n                self.column = 0\n            self.advance()\n            \n        # Skip the closing */\n        self.advance()  # Skip *\n        self.advance()  # Skip /\n        \n        # Get the docstring content (without /** and */)\n        content = self.source[self.start + 3:self.current - 2].strip()\n        self.add_token(TokenType.DOCSTRING, content)\n\n    def identifier(self):\n        \"\"\"Lex an identifier or keyword.\"\"\"\n        while self.peek().isalnum() or self.peek() == '_':\n            self.advance()\n            \n        # Check if the identifier is a keyword\n        text = self.source[self.start:self.current]\n        token_type = KEYWORDS.get(text, TokenType.IDENTIFIER)\n        self.add_token(token_type)\n\n    def error(self, message: str, line: Optional[int] = None, column: Optional[int] = None):\n        \"\"\"Report a lexing error.\"\"\"\n        line = line or self.line\n        column = column or self.column\n        length = self.current - self.start\n        self.errors.append(LexerError(\n            message=message,\n            line=line,\n            column=column,\n            length=max(1, length)  # At least length 1\n        ))\n        \n        # Try to recover by skipping to the next whitespace or known delimiter\n        while not self.is_at_end() and not self.peek().isspace() and self.peek() not in ';{}()[]':\n            self.advance()\n\n    def is_at_end(self) -> bool:\n        \"\"\"Check if we've reached the end of the source.\"\"\"\n        return self.current >= len(self.source)\n\n    def advance(self) -> str:\n        \"\"\"Consume and return the next character.\"\"\"\n        if self.is_at_end():\n            return '\\0'\n        char = self.source[self.current]\n        self.current += 1\n        self.column += 1\n        return char\n\n    def match(self, expected: str) -> bool:\n        \"\"\"Conditionally consume a character if it matches the expected value.\"\"\"\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.current += 1\n        self.column += 1\n        return True\n\n    def peek(self) -> str:\n        \"\"\"Look at the next character without consuming it.\"\"\"\n        if self.is_at_end():\n            return '\\0'\n        return self.source[self.current]\n\n    def peek_next(self) -> str:\n        \"\"\"Look at the character after the next one without consuming it.\"\"\"\n        if self.current + 1 >= len(self.source):\n            return '\\0'\n        return self.source[self.current + 1]\n\n    def add_token(self, token_type: TokenType, literal: Any = None):\n        \"\"\"Add a new token to the token list.\"\"\"\n        text = self.source[self.start:self.current]\n        self.tokens.append(Token(token_type, text, literal, self.line, self.column))",
  "metadata": {},
  "relative_path": "src\\core\\lexer.py",
  "id": "fd0bb31ef6ef690a5e6f503156863800"
}