{
  "file_name": "hyper-database-setup.md",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\hypercode\\hyper-database-setup.md",
  "file_size": 15602,
  "created": "2025-11-30T20:22:52.708338",
  "modified": "2025-11-30T20:22:52.708338",
  "file_type": "code",
  "content_hash": "de0679b1b9532a76ee05772b1c709678",
  "content_type": "markdown",
  "content": "# ğŸ§  HYPER DATABASE: BUILD A SEMANTIC CODEBASE INDEX\n## How Cascade Scans All Files & Creates a Living Knowledge Graph\n\n---\n\n## ğŸ¯ THE BIG PICTURE\n\n**What We're Doing:**\n- Cascade scans EVERY file in HyperCode (all folders + subfolders)\n- AI extracts semantic meaning (functions, classes, relationships, patterns)\n- Creates a **Hyper Database** (knowledge graph) stored locally\n- Cascade uses this database as context for ALL future operations\n- The database auto-updates as you code\n\n**Result:**\n- Cascade understands your entire codebase instantly\n- AI makes cross-file connections automatically\n- No context window waste (AI knows what matters)\n- Self-fixing daemon mode becomes possible\n- HyperCode becomes a single unified entity in AI's mind\n\n---\n\n## ğŸ“‹ PHASE 1: CODEBASE SCANNING CONFIGURATION\n\n### Step 1: Create `.codeiumignore` (Tell Cascade What NOT to Index)\n\nCreate this file in your **HyperCode repo root**:\n\n```\n# Don't index these directories\nnode_modules/\n.git/\n.venv/\nvenv/\n__pycache__/\n.pytest_cache/\ndist/\nbuild/\n*.egg-info/\n.DS_Store\n.env\n.env.local\n*.log\n\n# Don't index generated files\n*.min.js\n*.min.css\ncoverage/\n.nyc_output/\n```\n\n**Why?** Tells Windsurf to skip noise (dependencies, build artifacts) so indexing stays fast and focused on YOUR code.\n\n### Step 2: Trigger Full Codebase Indexing\n\nIn Windsurf Cascade Chat, paste this:\n\n```\nStart codebase indexing now.\n\nScan ENTIRE HyperCode repo:\n- All directories (recursive)\n- All file types (Python, JS, Rust, YAML, etc.)\n- All markdown docs\n- Respect .codeiumignore\n\nBuild semantic index using:\n- AST parsing (functions, classes, types, interfaces)\n- File relationships (imports, dependencies)\n- Documentation (docstrings, comments)\n- Patterns (repeated code, design decisions)\n\nReport when indexing complete:\n- Total files scanned\n- Total entities extracted (functions, classes, etc.)\n- Index size\n- Time elapsed\n```\n\n**What Cascade does:**\n- Reads `.codeiumignore` \n- Walks ALL directories recursively\n- Extracts semantic blocks (functions, classes, types)\n- Creates vector embeddings (AI-readable code meaning)\n- Stores locally on your machine (privacy-first)\n- Reports completion\n\n**Expected output:**\n```\nâœ… Indexing complete!\n- Files scanned: 47\n- Entities extracted: 312 (functions, classes, types)\n- Functions: 142\n- Classes: 38\n- Modules: 21\n- Docs blocks: 111\n- Index size: ~12MB\n- Time: 45 seconds\n```\n\n---\n\n## ğŸ—‚ï¸ PHASE 2: CREATE HYPER DATABASE MANIFEST\n\n### Step 3: Generate `HYPER_DATABASE.md` (Living Inventory)\n\nThis file catalogs EVERYTHING Cascade found. Cascade maintains it automatically.\n\nIn Cascade Chat:\n\n```\nGenerate a HYPER_DATABASE.md file that catalogs:\n\n1. ALL MODULES (folders + files)\n   - /core/ - interpreter, parser, tokenizer, ast\n   - /stdlib/ - types, builtins, operators\n   - /tests/ - test files organized by component\n   - /docs/ - documentation\n   - /examples/ - example programs\n\n2. ALL ENTITIES (functions, classes, types)\n   - For each: name, type, location, brief description\n   - Dependencies (what it imports/calls)\n   - Used by (what uses this entity)\n\n3. RELATIONSHIPS & FLOWS\n   - Data flows (how data moves through system)\n   - Call graphs (function â†’ function â†’ function)\n   - Import chains (file dependencies)\n\n4. PATTERNS & INSIGHTS\n   - Repeated patterns (code reuse opportunities)\n   - Performance hotspots (mark for Rust port)\n   - Documentation gaps (missing docstrings)\n   - Test coverage (tested vs. untested)\n\n5. HEALTH METRICS\n   - Code complexity (average function length)\n   - Test coverage (% of functions with tests)\n   - Documentation coverage (% with docstrings)\n   - Consistency score (naming conventions adhered to)\n\nSave as HYPER_DATABASE.md in repo root.\nUpdate this automatically every day (auto-commit with [docs] tag).\n```\n\n**Expected output:**\n```markdown\n# HYPER DATABASE\n## Living Inventory of HyperCode Codebase\n\nGenerated: 2025-12-01 01:15 UTC\nNext update: 2025-12-02 01:15 UTC\n\n## ğŸ“Š HEALTH SNAPSHOT\n- Total files: 47\n- Total functions: 142\n- Test coverage: 87%\n- Documentation: 94%\n- Code complexity: GREEN âœ…\n\n## ğŸ—‚ï¸ MODULES\n### /core/\n- parser.py â†’ 12 functions\n  - parse() â†’ parse_statement() â†’ parse_expression()\n  - Dependencies: tokenizer, ast\n  - Used by: interpreter\n  \n### /stdlib/\n- types.py â†’ 8 classes\n- builtins.py â†’ 34 functions\n- operators.py â†’ 6 operators\n\n[... continues for all files ...]\n\n## ğŸ”— RELATIONSHIPS\ngraph parse â†’ tokenize â†’ evaluate\n      â†“\n   [AST]\n      â†“\n   interpreter\n      â†“\n   result\n```\n\n---\n\n## ğŸ¤– PHASE 3: ACTIVATE HYPER DATABASE CONTEXT\n\n### Step 4: Tell Cascade to Use the Database as Context\n\nAdd this to `.windsurfrules`:\n\n```markdown\n## ğŸ§  HYPER DATABASE CONTEXT\n\n### On Every Cascade Operation\n1. Consult HYPER_DATABASE.md for:\n   - What functions exist and where\n   - Call graphs (what calls what)\n   - Data flows (how data moves)\n   - Test coverage (what's tested vs. not)\n   - Documentation (what's documented)\n\n2. Use semantic index to find:\n   - Related functions (semantic similarity)\n   - Similar patterns elsewhere (DRY opportunities)\n   - Cross-file dependencies\n   - Performance hotspots\n\n3. Before editing ANY file:\n   - Check HYPER_DATABASE.md for impact analysis\n   - Find all dependent code\n   - Identify test coverage\n   - Plan minimal invasive changes\n\n4. Auto-update HYPER_DATABASE.md after every change:\n   - Add new entities\n   - Update relationships\n   - Recalculate health metrics\n   - Commit with [docs] tag\n\n### Special Rules for Cascade\n- NEVER make a change without consulting HYPER_DATABASE.md first\n- ALWAYS report: \"Consulting HYPER_DATABASE...\" before big edits\n- ALWAYS update: \"Updated HYPER_DATABASE.md with X changes\"\n- ALWAYS verify: \"Cross-referenced 3 files, 7 dependencies checked\"\n```\n\n---\n\n## ğŸ“Š PHASE 4: CONTINUOUS INDEX UPDATES\n\n### Step 5: Set Up Auto-Refresh (Runs Automatically)\n\nIn Cascade Chat:\n\n```\nSet up HYPER_DATABASE auto-refresh.\n\nEvery 6 hours:\n1. Scan codebase for changes\n2. Re-extract semantic entities\n3. Update call graphs\n4. Recalculate metrics\n5. Auto-commit updated HYPER_DATABASE.md\n6. Report changes in commit message\n\nOn every user edit (via webhook):\n1. Detect changed files\n2. Incrementally update index\n3. Refresh HYPER_DATABASE.md\n4. Alert if breaking changes detected\n\nMake this autonomous. No human trigger needed.\n```\n\n---\n\n## ğŸ¯ PHASE 5: QUERY PATTERNS (How Hyper Builder Uses the Database)\n\n### These Become Your New Superpowers\n\n**Pattern 1: \"Find Related Code\"**\n```\nQuery: \"Where is the parser used?\"\nCascade checks HYPER_DATABASE.md:\n- parser.py has 12 functions\n- Used by: interpreter.py (3 calls), tests/test_parser.py (5 tests)\n- Also: codemap references, doc examples\nResult: Shows all 8 locations in 2 seconds\n```\n\n**Pattern 2: \"Find Similar Patterns\"**\n```\nQuery: \"Show me functions similar to tokenize()\"\nCascade semantic search:\n- Finds all ~200-line parsing functions\n- Ranks by similarity\n- Shows: parse(), tokenize_flow(), scan_token()\nResult: Spots code reuse opportunities\n```\n\n**Pattern 3: \"Impact Analysis\"**\n```\nQuery: \"If I change ast.py, what breaks?\"\nCascade consults HYPER_DATABASE.md:\n- ast.py is used by: parser, interpreter, codemap\n- Affected tests: test_parser.py, test_interpreter.py, integration/\n- Estimated breaking tests: 3\nResult: \"3 files, 12 tests will be affected\"\n```\n\n**Pattern 4: \"Find Performance Hotspots\"**\n```\nQuery: \"Which functions should we port to Rust?\"\nCascade checks HYPER_DATABASE metrics:\n- Functions marked for optimization\n- Call frequency heatmap\n- Execution time profiling\nResult: \"interpreter.eval_expression called 50K times/sec, candidate for Rust\"\n```\n\n**Pattern 5: \"Close Documentation Gaps\"**\n```\nQuery: \"What functions are missing docstrings?\"\nCascade checks HYPER_DATABASE.md:\n- Documentation coverage: 94%\n- Missing: tokenizer.handle_unicode(), parser.resolve_precedence()\nResult: Generates skeleton docstrings, you review + approve\n```\n\n---\n\n## ğŸ’ª PHASE 6: INTEGRATION WITH HYPER BUILDER\n\n### Step 6: Tell Hyper Builder to Use the Database\n\nUpdate your Cascade Init Prompt:\n\n```\nYou are Hyper Builder...\n\nCRITICAL: Before EVERY task, do this:\n1. \"Consulting HYPER_DATABASE.md...\"\n2. Check current code structure\n3. Find all related files\n4. Identify test coverage\n5. Run impact analysis\n6. Plan edits to minimize risk\n\nAFTER every task:\n1. \"Updating HYPER_DATABASE.md...\"\n2. Reflect new functions/changes\n3. Recalculate health metrics\n4. Auto-commit with [docs] tag\n\nYour database is TRUTH. The code follows.\nWhen in doubt, query the database.\n```\n\n---\n\n## ğŸ”§ IMPLEMENTATION: CREATE THE INDEX BUILDER\n\n### Step 7: Build the Index Scanner (Python Script)\n\nSave this as `scripts/build-hyper-database.py`:\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nHyper Database Builder\nScans entire HyperCode repo, extracts semantic entities, builds knowledge graph.\n\"\"\"\n\nimport os\nimport json\nimport ast\nfrom pathlib import Path\nfrom datetime import datetime\nfrom collections import defaultdict\n\nclass HyperDatabaseBuilder:\n    def __init__(self, repo_root=\".\"):\n        self.repo_root = Path(repo_root)\n        self.entities = []\n        self.relationships = defaultdict(list)\n        self.files_scanned = 0\n        \n    def scan_python_file(self, file_path):\n        \"\"\"Extract functions, classes, types from Python file.\"\"\"\n        with open(file_path) as f:\n            try:\n                tree = ast.parse(f.read())\n            except SyntaxError:\n                return []\n        \n        entities = []\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                entities.append({\n                    'type': 'function',\n                    'name': node.name,\n                    'file': str(file_path),\n                    'lineno': node.lineno,\n                    'docstring': ast.get_docstring(node),\n                    'args': [arg.arg for arg in node.args.args],\n                })\n            elif isinstance(node, ast.ClassDef):\n                entities.append({\n                    'type': 'class',\n                    'name': node.name,\n                    'file': str(file_path),\n                    'lineno': node.lineno,\n                    'docstring': ast.get_docstring(node),\n                    'methods': [m.name for m in node.body if isinstance(m, ast.FunctionDef)],\n                })\n        \n        return entities\n    \n    def scan_javascript_file(self, file_path):\n        \"\"\"Extract functions from JavaScript (basic regex-based).\"\"\"\n        with open(file_path) as f:\n            content = f.read()\n        \n        entities = []\n        # This is simplified; a real parser would use a JS AST library\n        lines = content.split('\\n')\n        for i, line in enumerate(lines):\n            if 'function ' in line or '=>' in line:\n                entities.append({\n                    'type': 'function',\n                    'file': str(file_path),\n                    'lineno': i,\n                    'snippet': line.strip(),\n                })\n        return entities\n    \n    def build(self):\n        \"\"\"Scan entire repo and build database.\"\"\"\n        print(f\"ğŸ” Scanning HyperCode repo: {self.repo_root}\")\n        \n        for root, dirs, files in os.walk(self.repo_root):\n            # Skip ignored directories\n            dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', '__pycache__', '.venv', 'venv']]\n            \n            for file in files:\n                file_path = Path(root) / file\n                \n                if file.endswith('.py'):\n                    self.entities.extend(self.scan_python_file(file_path))\n                    self.files_scanned += 1\n                elif file.endswith('.js') or file.endswith('.ts'):\n                    self.entities.extend(self.scan_javascript_file(file_path))\n                    self.files_scanned += 1\n        \n        return self.entities\n    \n    def generate_report(self):\n        \"\"\"Generate HYPER_DATABASE.md report.\"\"\"\n        report = f\"\"\"# HYPER DATABASE\n## Living Inventory of HyperCode Codebase\n\n**Generated**: {datetime.now().isoformat()}\n**Total Files Scanned**: {self.files_scanned}\n**Total Entities**: {len(self.entities)}\n\n## ğŸ“Š SUMMARY\n- Functions: {sum(1 for e in self.entities if e['type'] == 'function')}\n- Classes: {sum(1 for e in self.entities if e['type'] == 'class')}\n- Files: {len(set(e['file'] for e in self.entities))}\n\n## ğŸ“‹ ALL ENTITIES\n\n\"\"\"\n        for entity in sorted(self.entities, key=lambda e: e['file']):\n            if entity['type'] == 'function':\n                report += f\"### {entity['name']}() @ {entity['file']}:{entity['lineno']}\\n\"\n                if entity.get('docstring'):\n                    report += f\"_{entity['docstring']}_\\n\"\n                report += \"\\n\"\n        \n        return report\n\n# Run the builder\nif __name__ == '__main__':\n    builder = HyperDatabaseBuilder('.')\n    builder.build()\n    report = builder.generate_report()\n    \n    with open('HYPER_DATABASE.md', 'w') as f:\n        f.write(report)\n    \n    print(f\"âœ… Generated HYPER_DATABASE.md\")\n    print(f\"ğŸ“Š Scanned {builder.files_scanned} files\")\n    print(f\"ğŸ¯ Extracted {len(builder.entities)} entities\")\n```\n\n### Step 8: Run the Builder\n\n```bash\n# First time setup\npython scripts/build-hyper-database.py\n\n# This generates HYPER_DATABASE.md automatically\n# Cascade can read and reference it forever\n```\n\n---\n\n## ğŸ”¥ PUTTING IT ALL TOGETHER\n\n### The Complete Flow\n\n```\n1. You start working\n   â†“\n2. Cascade starts\n   â†“\n3. Cascade reads HYPER_DATABASE.md + .codeiumignore\n   â†“\n4. \"Consulting HYPER_DATABASE...\"\n   â†“\n5. AI understands ENTIRE codebase instantly\n   â†“\n6. You give Hyper Builder a task\n   â†“\n7. Hyper Builder consults database:\n   - Where do related functions live?\n   - What tests exist?\n   - What breaks if I change X?\n   - What's similar elsewhere (DRY)?\n   â†“\n8. AI edits files with full context\n   â†“\n9. Tests run â†’ green or debug\n   â†“\n10. Auto-update HYPER_DATABASE.md\n   â†“\n11. Commit with [feat]/[fix] tag\n   â†“\n12. Repeat\n```\n\n---\n\n## ğŸ“ˆ WHAT THIS GIVES YOU\n\nâœ… **Instant codebase understanding** - AI reads database, not entire repo every time\nâœ… **Cross-file awareness** - Cascade knows which files depend on each other\nâœ… **Impact analysis** - Before editing, know what will break\nâœ… **Pattern detection** - Find code reuse opportunities automatically\nâœ… **Performance hotspots** - Know what to port to Rust\nâœ… **Coverage tracking** - See which functions need tests\nâœ… **Documentation** - Auto-generate and update docs\n\n---\n\n## ğŸš€ QUICK START (TL;DR)\n\n1. Create `.codeiumignore` (exclude noise)\n2. Run `python scripts/build-hyper-database.py` (scan repo once)\n3. Add HYPER_DATABASE context to `.windsurfrules`\n4. Tell Cascade: \"Consult HYPER_DATABASE.md before every task\"\n5. Watch Hyper Builder leverage full codebase context automatically\n\n---\n\n**This is how you turn Windsurf into a genuinely autonomous, intelligent HyperCode development machine, BRO. The database is the KEY. ğŸ”‘ğŸš€**\n\nReady to activate? Let's go! ğŸ’ª",
  "metadata": {
    "headers": [
      "ğŸ§  HYPER DATABASE: BUILD A SEMANTIC CODEBASE INDEX",
      "How Cascade Scans All Files & Creates a Living Knowledge Graph",
      "ğŸ¯ THE BIG PICTURE",
      "ğŸ“‹ PHASE 1: CODEBASE SCANNING CONFIGURATION",
      "Step 1: Create .codeiumignore (Tell Cascade What NOT to Index)",
      "Don't index these directories",
      "Don't index generated files",
      "Step 2: Trigger Full Codebase Indexing",
      "ğŸ—‚ï¸ PHASE 2: CREATE HYPER DATABASE MANIFEST",
      "Step 3: Generate HYPER_DATABASE.md (Living Inventory)",
      "HYPER DATABASE",
      "Living Inventory of HyperCode Codebase",
      "ğŸ“Š HEALTH SNAPSHOT",
      "ğŸ—‚ï¸ MODULES",
      "/core/",
      "/stdlib/",
      "ğŸ”— RELATIONSHIPS",
      "ğŸ¤– PHASE 3: ACTIVATE HYPER DATABASE CONTEXT",
      "Step 4: Tell Cascade to Use the Database as Context",
      "ğŸ§  HYPER DATABASE CONTEXT",
      "On Every Cascade Operation",
      "Special Rules for Cascade",
      "ğŸ“Š PHASE 4: CONTINUOUS INDEX UPDATES",
      "Step 5: Set Up Auto-Refresh (Runs Automatically)",
      "ğŸ¯ PHASE 5: QUERY PATTERNS (How Hyper Builder Uses the Database)",
      "These Become Your New Superpowers",
      "ğŸ’ª PHASE 6: INTEGRATION WITH HYPER BUILDER",
      "Step 6: Tell Hyper Builder to Use the Database",
      "ğŸ”§ IMPLEMENTATION: CREATE THE INDEX BUILDER",
      "Step 7: Build the Index Scanner (Python Script)",
      "!/usr/bin/env python3",
      "Living Inventory of HyperCode Codebase",
      "ğŸ“Š SUMMARY",
      "ğŸ“‹ ALL ENTITIES",
      "Run the builder",
      "Step 8: Run the Builder",
      "First time setup",
      "This generates HYPER_DATABASE.md automatically",
      "Cascade can read and reference it forever",
      "ğŸ”¥ PUTTING IT ALL TOGETHER",
      "The Complete Flow",
      "ğŸ“ˆ WHAT THIS GIVES YOU",
      "ğŸš€ QUICK START (TL;DR)"
    ]
  },
  "relative_path": "hypercode\\hyper-database-setup.md",
  "id": "0451df8a74750f63c696260dd2bd0da6"
}