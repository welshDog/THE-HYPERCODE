{
  "file_name": "lexer.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\src\\core\\lexer.py",
  "file_size": 15292,
  "created": "2025-11-15T15:54:41.042314",
  "modified": "2025-11-26T02:00:12.351590",
  "file_type": "code",
  "content_hash": "072cb4fca79e6880290741d2e9276a85",
  "content_type": "text",
  "content": "\"\"\"\nHyperCode Lexer Module\n\nTokenizes HyperCode source code into a stream of tokens for parsing.\nHandles keywords, identifiers, literals, operators, and special syntax.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\n\nfrom .tokens import Token, TokenType\n\n\n@dataclass\nclass LexerError:\n    \"\"\"\n    Represents a lexical analysis error.\n\n    Attributes:\n        message: Description of the error\n        line: Line number where the error occurred (1-based)\n        column: Column number where the error occurred (1-based)\n        length: Length of the problematic token (default: 1)\n    \"\"\"\n\n    message: str\n    line: int\n    column: int\n    length: int = 1\n\n\nclass Lexer:\n    \"\"\"\n    Lexical analyzer for the HyperCode programming language.\n\n    Converts source code into a sequence of tokens that can be parsed.\n    Handles syntax highlighting, error reporting, and source mapping.\n\n    Attributes:\n        source: The source code to tokenize\n        tokens: List of tokens generated from the source\n        errors: List of lexing errors encountered\n        start: Starting position of the current lexeme\n        current: Current position in the source\n        line: Current line number (1-based)\n        column: Current column number (1-based)\n    \"\"\"\n\n    def __init__(self, source: str):\n        \"\"\"\n        Initialize the lexer with source code.\n\n        Args:\n            source: The source code string to tokenize\n        \"\"\"\n        self.source = source\n        self.tokens: List[Token] = []\n        self.errors: List[LexerError] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n\n        # Keywords map to their corresponding token types\n        self.KEYWORDS: Dict[str, TokenType] = {\n            # Control flow\n            \"if\": TokenType.IF,\n            \"else\": TokenType.ELSE,\n            \"for\": TokenType.FOR,\n            \"while\": TokenType.WHILE,\n            \"match\": TokenType.MATCH,\n            \"case\": TokenType.CASE,\n            \"default\": TokenType.DEFAULT,\n            # Functions\n            \"fun\": TokenType.FUN,\n            \"return\": TokenType.RETURN,\n            \"yield\": TokenType.YIELD,\n            \"await\": TokenType.AWAIT,\n            # Variables\n            \"let\": TokenType.LET,\n            \"const\": TokenType.CONST,\n            \"var\": TokenType.VAR,\n            # I/O\n            \"print\": TokenType.PRINT,\n            \"input\": TokenType.INPUT,\n            # Literals\n            \"true\": TokenType.TRUE,\n            \"false\": TokenType.FALSE,\n            \"nil\": TokenType.NIL,\n            # OOP\n            \"class\": TokenType.CLASS,\n            \"interface\": TokenType.INTERFACE,\n            \"extends\": TokenType.EXTENDS,\n            \"implements\": TokenType.IMPLEMENTS,\n            \"this\": TokenType.THIS,\n            \"super\": TokenType.SUPER,\n            \"new\": TokenType.NEW,\n            # Modules\n            \"import\": TokenType.IMPORT,\n            \"export\": TokenType.EXPORT,\n            \"from\": TokenType.FROM,\n            \"as\": TokenType.AS,\n            # Error handling\n            \"try\": TokenType.TRY,\n            \"catch\": TokenType.CATCH,\n            \"finally\": TokenType.FINALLY,\n            \"throw\": TokenType.THROW,\n        }\n\n    def tokenize(self) -> List[Token]:\n        \"\"\"\n        Convert the source code into a list of tokens.\n\n        Returns:\n            List of tokens representing the source code.\n\n        Example:\n            >>> lexer = Lexer(\"let x = 42\")\n            >>> tokens = lexer.tokenize()\n            >>> [t.type for t in tokens]\n            [TokenType.LET, TokenType.IDENTIFIER, TokenType.EQUAL, TokenType.NUMBER, TokenType.EOF]\n        \"\"\"\n        while not self.is_at_end():\n            self.start = self.current\n            try:\n                self.scan_token()\n            except Exception as e:\n                self.error(f\"Unexpected character: {e}\")\n                self.synchronize()\n\n        self.tokens.append(Token(TokenType.EOF, \"\", None, self.line, self.column))\n        return self.tokens\n\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n\n    def scan_token(self) -> None:\n        \"\"\"\n        Scan the next token from the source code.\n\n        This method processes a single token and adds it to the tokens list.\n        It handles all token types including keywords, identifiers, literals,\n        and operators.\n\n        Raises:\n            ValueError: If an unexpected character is encountered\n        \"\"\"\n        char = self.advance()\n\n        # Skip whitespace\n        if char in \" \\r\\t\":\n            return\n\n        # Handle newlines\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n            return\n\n        # Handle line continuation\n        if char == \"\\\\\" and self.peek() == \"\\n\":\n            self.advance()  # Consume the newline\n            self.line += 1\n            self.column = 1\n            return\n\n        # Single-character tokens\n        if char == \"(\":\n            self.add_token(TokenType.LPAREN)\n        elif char == \")\":\n            self.add_token(TokenType.RPAREN)\n        elif char == \"{\":\n            self.add_token(TokenType.LBRACE)\n        elif char == \"}\":\n            self.add_token(TokenType.RBRACE)\n        elif char == \"[\":\n            self.add_token(TokenType.LBRACKET)\n        elif char == \"]\":\n            self.add_token(TokenType.RBRACKET)\n        elif char == \",\":\n            self.add_token(TokenType.COMMA)\n        elif char == \".\":\n            self.add_token(TokenType.DOT)\n        elif char == \"-\":\n            self.add_token(TokenType.MINUS)\n        elif char == \"+\":\n            self.add_token(TokenType.PLUS)\n        elif char == \";\":\n            self.add_token(TokenType.SEMICOLON)\n        elif char == \":\":\n            self.add_token(TokenType.COLON)\n        elif char == \"*\":\n            self.add_token(TokenType.STAR)\n\n        # Two-character tokens\n        elif char == \"!\":\n            self.add_token(TokenType.BANG_EQUAL if self.match(\"=\") else TokenType.BANG)\n        elif char == \"=\":\n            self.add_token(\n                TokenType.EQUAL_EQUAL if self.match(\"=\") else TokenType.EQUAL\n            )\n        elif char == \"<\":\n            self.add_token(TokenType.LESS_EQUAL if self.match(\"=\") else TokenType.LESS)\n        elif char == \">\":\n            self.add_token(\n                TokenType.GREATER_EQUAL if self.match(\"=\") else TokenType.GREATER\n            )\n\n        # Comments\n        elif char == \"/\":\n            if self.match(\"/\"):\n                while self.peek() != \"\\n\" and not self.is_at_end():\n                    self.advance()\n            else:\n                self.add_token(TokenType.SLASH)\n\n        # Literals\n        elif char == '\"':\n            self.string()\n        elif self.is_digit(char):\n            self.number()\n        elif self.is_alpha(char):\n            self.identifier()\n\n        else:\n            self.add_token(TokenType.UNKNOWN)\n\n    def advance(self) -> str:\n        self.current += 1\n        self.column += 1\n        return self.source[self.current - 1]\n\n    def add_token(self, type: TokenType, literal: Any = None) -> None:\n        \"\"\"\n        Add a new token to the tokens list.\n\n        Args:\n            type: The token type\n            literal: The literal value (for numbers, strings, etc.)\n        \"\"\"\n        text = self.source[self.start : self.current]\n        self.tokens.append(\n            Token(type, text, literal, self.line, self.column - len(text))\n        )\n\n    def error(self, message: str) -> None:\n        \"\"\"\n        Record a lexing error.\n\n        Args:\n            message: Error message\n        \"\"\"\n        self.errors.append(\n            LexerError(\n                message=message,\n                line=self.line,\n                column=self.column - (self.current - self.start),\n                length=self.current - self.start,\n            )\n        )\n\n    def synchronize(self) -> None:\n        \"\"\"\n        Synchronize after an error by skipping tokens until we find a statement boundary.\n        \"\"\"\n        while not self.is_at_end():\n            if self.previous() in \";\\n}\":\n                return\n            if self.peek() in \";\\n}\":\n                self.advance()\n                return\n            self.advance()\n\n    def previous(self) -> str:\n        \"\"\"Return the previous character.\"\"\"\n        if self.current == 0:\n            return \"\\0\"\n        return self.source[self.current - 1]\n\n    def peek_next(self) -> str:\n        \"\"\"Look ahead two characters.\"\"\"\n        if self.current + 1 >= len(self.source):\n            return \"\\0\"\n        return self.source[self.current + 1]\n\n    def match(self, expected: str) -> bool:\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.current += 1\n        self.column += 1\n        return True\n\n    def peek(self) -> str:\n        if self.is_at_end():\n            return \"\\0\"\n        return self.source[self.current]\n\n    def string(self) -> None:\n        \"\"\"Parse a string literal.\"\"\"\n        delimiter = self.previous()  # Should be '\"'\n        value = []\n\n        while True:\n            if self.is_at_end():\n                self.error(\"Unterminated string\")\n                return\n\n            char = self.advance()\n\n            if char == \"\\\\\":\n                # Handle escape sequences\n                if self.is_at_end():\n                    self.error(\"Unterminated escape sequence\")\n                    return\n\n                # Process escape sequence\n                esc = self.advance()\n                if esc == \"n\":\n                    value.append(\"\\n\")\n                elif esc == \"t\":\n                    value.append(\"\\t\")\n                elif esc == \"r\":\n                    value.append(\"\\r\")\n                elif esc == \"b\":\n                    value.append(\"\\b\")\n                elif esc == \"f\":\n                    value.append(\"\\f\")\n                elif esc in \"\\\"'\\\\\":\n                    value.append(esc)\n                elif esc == \"x\":\n                    # Hex escape: \\xHH\n                    if not (\n                        self.is_hex_digit(self.peek())\n                        and self.is_hex_digit(self.peek_next())\n                    ):\n                        self.error(\"Invalid hex escape sequence\")\n                        return\n                    hex_str = self.advance() + self.advance()\n                    value.append(chr(int(hex_str, 16)))\n                else:\n                    self.error(f\"Invalid escape sequence: \\\\{esc}\")\n                    value.append(esc)\n\n            elif char == delimiter:\n                break\n\n            elif char == \"\\n\":\n                if delimiter == '\"':\n                    self.error(\"Unterminated string\")\n                    return\n                self.line += 1\n                self.column = 1\n                value.append(\"\\n\")\n\n            else:\n                value.append(char)\n\n        self.add_token(TokenType.STRING, \"\".join(value))\n\n    def is_digit(self, char: str) -> bool:\n        \"\"\"Check if a character is a digit (0-9).\"\"\"\n        return char.isdigit()\n\n    def number(self) -> None:\n        \"\"\"Parse a number literal (integer or float).\"\"\"\n        # Handle hexadecimal numbers (0x...)\n        if self.peek() == \"0\" and self.peek_next().lower() == \"x\":\n            self.advance()  # Consume 'x'\n            self.advance()  # Consume first digit after 'x'\n            while self.is_hex_digit(self.peek()):\n                self.advance()\n            hex_str = self.source[self.start + 2 : self.current]\n            self.add_token(TokenType.NUMBER, int(hex_str, 16))\n            return\n\n        # Handle binary numbers (0b...)\n        if self.peek() == \"0\" and self.peek_next().lower() == \"b\":\n            self.advance()  # Consume 'b'\n            self.advance()  # Consume first digit after 'b'\n            while self.peek() in \"01_\":\n                if self.peek() != \"_\":\n                    self.advance()\n            bin_str = self.source[self.start + 2 : self.current].replace(\"_\", \"\")\n            self.add_token(TokenType.NUMBER, int(bin_str, 2))\n            return\n\n        # Handle decimal numbers\n        while self.peek().isdigit() or self.peek() == \"_\":\n            self.advance()\n\n        # Look for a fractional part\n        if self.peek() == \".\" and self.is_digit(self.peek_next()):\n            # Consume the \".\"\n            self.advance()\n\n            while self.peek().isdigit() or self.peek() == \"_\":\n                if self.peek() != \"_\":\n                    self.advance()\n\n        # Handle scientific notation\n        if self.peek().lower() == \"e\":\n            self.advance()\n            if self.peek() in \"+-\":\n                self.advance()\n            while self.peek().isdigit():\n                self.advance()\n\n        # Parse the number, ignoring underscores\n        num_str = self.source[self.start : self.current].replace(\"_\", \"\")\n        self.add_token(\n            TokenType.NUMBER,\n            (\n                float(num_str)\n                if \".\" in num_str or \"e\" in num_str.lower()\n                else int(num_str)\n            ),\n        )\n\n    def is_alpha(self, char: str) -> bool:\n        \"\"\"Check if a character is alphabetic or underscore.\"\"\"\n        return char.isalpha() or char == \"_\" or char == \"$\"\n\n    def is_alphanumeric(self, char: str) -> bool:\n        \"\"\"Check if a character is alphanumeric or underscore.\"\"\"\n        return self.is_alpha(char) or self.is_digit(char)\n\n    def is_hex_digit(self, char: str) -> bool:\n        \"\"\"Check if a character is a valid hexadecimal digit.\"\"\"\n        return char.isdigit() or char.lower() in \"abcdef\"\n\n    def identifier(self) -> None:\n        \"\"\"Parse an identifier or keyword.\"\"\"\n        while self.is_alphanumeric(self.peek()):\n            self.advance()\n\n        text = self.source[self.start : self.current]\n\n        # Check for contextual keywords (not reserved in all contexts)\n        if text in self.KEYWORDS:\n            token_type = self.KEYWORDS[text]\n        else:\n            token_type = TokenType.IDENTIFIER\n\n        self.add_token(token_type)\n\n        # Handle special cases like 'not in', 'is not'\n        if (\n            text == \"is\"\n            and self.peek() == \" \"\n            and self.source.startswith(\"not\", self.current + 1)\n        ):\n            # Handle 'is not' operator\n            lookahead = self.current + 1\n            while lookahead < len(self.source) and self.source[lookahead].isspace():\n                lookahead += 1\n\n            if self.source.startswith(\"not\", lookahead):\n                # Replace 'is' with 'is not' token\n                self.tokens.pop()  # Remove the 'is' token\n                self.current = lookahead + 3  # Skip 'not'\n                self.add_token(TokenType.IS_NOT)\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\src\\core\\lexer.py",
  "id": "f9fbf50f6a846773a669339d86ed1b88"
}