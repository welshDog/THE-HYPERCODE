{
  "file_name": "Week1-Sprint-Guide.md",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\hypercode\\docs\\Week1-Sprint-Guide.md",
  "file_size": 19032,
  "created": "2025-11-26T23:37:35.443378",
  "modified": "2025-11-26T23:37:35.443378",
  "file_type": "code",
  "content_hash": "458581a53e27a299ee3bddc958c9d50c",
  "content_type": "markdown",
  "content": "# ðŸš€ HyperCode: Week 1 Sprint Battle Plan\n\n## The ACTUAL First Steps (Right Now Energy!)\n\n**Status**: YOUR REPO IS LIVE! ðŸ”¥ **Mission**: Get Week 1 LOCKED IN **Timeline**: This\nweek â†’ Production-ready lexer\n\n---\n\n## ðŸŽ¯ TODAY (Tuesday Nov 11, 2025)\n\n### IMMEDIATE ACTIONS (Next 30 min)\n\n#### 1. Setup Your Local Environment\n\n```bash\n# Clone YOUR repo\ngit clone https://github.com/welshDog/hypercode.git\ncd hypercode\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies (from our build guide)\npip install -r requirements.txt\npip install -r requirements-dev.txt\n\n# Verify setup\npython --version  # Should be 3.10+\npytest --version\n```\n\n#### 2. Create the Core Lexer File\n\n```bash\n# Make sure you're in the right place\nls -la core/  # Should exist from setup\n\n# Create core/lexer.py with this EXACT content:\n```\n\n**core/lexer.py**:\n\n```python\n\"\"\"\nHyperCode Lexer (Tokenizer)\nConverts HyperCode source â†’ Token stream\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nfrom enum import Enum\n\nclass TokenType(Enum):\n    \"\"\"HyperCode token types - inspired by Brainfuck minimalism\"\"\"\n    PUSH = \"PUSH\"           # > move pointer right\n    POP = \"POP\"             # < move pointer left\n    INCR = \"INCR\"           # + increment cell\n    DECR = \"DECR\"           # - decrement cell\n    OUTPUT = \"OUTPUT\"       # . output character\n    INPUT = \"INPUT\"         # , read character\n    LOOP_START = \"LOOP_START\"   # [ start loop\n    LOOP_END = \"LOOP_END\"       # ] end loop\n    SPATIAL_2D = \"SPATIAL_2D\"   # @ enter 2D mode (Befunge-style)\n    AI_NATIVE = \"AI_NATIVE\"     # # AI-native mode marker\n    COMMENT = \"COMMENT\"         # ; ignore rest of line\n    UNKNOWN = \"UNKNOWN\"\n\n\n@dataclass\nclass Token:\n    \"\"\"Represents a single token\"\"\"\n    type: TokenType\n    value: str\n    position: int\n    line: int\n    column: int\n\n\nclass HyperCodeLexer:\n    \"\"\"\n    Tokenizes HyperCode programs.\n\n    Minimal 8-core operations + extensions:\n    > < + - . , [ ]    (Brainfuck core)\n    @                   (2D mode)\n    #                   (AI-native)\n    ;                   (comments)\n    \"\"\"\n\n    TOKEN_MAP = {\n        '>': TokenType.PUSH,\n        '<': TokenType.POP,\n        '+': TokenType.INCR,\n        '-': TokenType.DECR,\n        '.': TokenType.OUTPUT,\n        ',': TokenType.INPUT,\n        '[': TokenType.LOOP_START,\n        ']': TokenType.LOOP_END,\n        '@': TokenType.SPATIAL_2D,\n        '#': TokenType.AI_NATIVE,\n        ';': TokenType.COMMENT,\n    }\n\n    def __init__(self):\n        self.source = \"\"\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n\n    def tokenize(self, source: str) -> List[Token]:\n        \"\"\"\n        Convert HyperCode source to token stream.\n\n        Args:\n            source: Raw HyperCode program text\n\n        Returns:\n            List of Token objects\n        \"\"\"\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n\n        while self.position < len(self.source):\n            char = self.source[self.position]\n\n            # Handle comments\n            if char == ';':\n                self._skip_comment()\n                continue\n\n            # Handle whitespace\n            if char.isspace():\n                self._advance_position(char)\n                continue\n\n            # Handle tokens\n            if char in self.TOKEN_MAP:\n                token_type = self.TOKEN_MAP[char]\n                token = Token(\n                    type=token_type,\n                    value=char,\n                    position=self.position,\n                    line=self.line,\n                    column=self.column\n                )\n                self.tokens.append(token)\n            else:\n                # Unknown character - create UNKNOWN token or skip\n                token = Token(\n                    type=TokenType.UNKNOWN,\n                    value=char,\n                    position=self.position,\n                    line=self.line,\n                    column=self.column\n                )\n                self.tokens.append(token)\n\n            self._advance_position(char)\n\n        return self.tokens\n\n    def _advance_position(self, char: str):\n        \"\"\"Update position tracking after processing character\"\"\"\n        self.position += 1\n        self.column += 1\n\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n\n    def _skip_comment(self):\n        \"\"\"Skip until end of line\"\"\"\n        while self.position < len(self.source) and self.source[self.position] != '\\n':\n            self._advance_position(self.source[self.position])\n\n    def get_tokens(self) -> List[Token]:\n        \"\"\"Return current token list\"\"\"\n        return self.tokens\n\n    def filter_tokens(self, exclude_types: List[TokenType] = None) -> List[Token]:\n        \"\"\"\n        Get tokens excluding certain types (e.g., UNKNOWN).\n\n        Args:\n            exclude_types: Token types to exclude\n\n        Returns:\n            Filtered token list\n        \"\"\"\n        if exclude_types is None:\n            exclude_types = [TokenType.UNKNOWN]\n\n        return [t for t in self.tokens if t.type not in exclude_types]\n\n\n# CLI Usage Example\nif __name__ == \"__main__\":\n    # Example HyperCode program: print \"Hi\"\n    program = \"\"\"\n    ; Print Hello in HyperCode\n    ++++++++  ; H = 72\n    >++++++++++  ; i = 105\n    <<.>.\n    \"\"\"\n\n    lexer = HyperCodeLexer()\n    tokens = lexer.tokenize(program)\n\n    print(f\"Found {len(tokens)} tokens:\")\n    for token in tokens:\n        if token.type != TokenType.COMMENT:\n            print(f\"  {token.type.value:15} | {token.value!r:5} | L{token.line}:C{token.column}\")\n```\n\n#### 3. Create Tests\n\n**tests/test_lexer.py**:\n\n```python\n\"\"\"\nUnit tests for HyperCode Lexer\n\"\"\"\n\nimport pytest\nfrom core.lexer import HyperCodeLexer, TokenType, Token\n\n\nclass TestLexerBasic:\n    \"\"\"Test basic tokenization\"\"\"\n\n    def test_empty_source(self):\n        \"\"\"Lexer handles empty source\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"\")\n        assert len(tokens) == 0\n\n    def test_single_tokens(self):\n        \"\"\"Lexer recognizes core 8 tokens\"\"\"\n        lexer = HyperCodeLexer()\n\n        token_chars = \"><+-.,\"\n        for char in token_chars:\n            tokens = lexer.tokenize(char)\n            assert len(tokens) == 1\n            assert tokens[0].value == char\n\n    def test_loop_tokens(self):\n        \"\"\"Lexer recognizes loop syntax\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"[+]\")\n\n        assert len(tokens) == 3\n        assert tokens[0].type == TokenType.LOOP_START\n        assert tokens[1].type == TokenType.INCR\n        assert tokens[2].type == TokenType.LOOP_END\n\n    def test_multiple_operations(self):\n        \"\"\"Lexer handles sequences\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+++>.\")\n\n        assert len(tokens) == 5\n        assert tokens[0].type == TokenType.INCR\n        assert tokens[1].type == TokenType.INCR\n        assert tokens[2].type == TokenType.INCR\n        assert tokens[3].type == TokenType.PUSH\n        assert tokens[4].type == TokenType.OUTPUT\n\n\nclass TestLexerWhitespace:\n    \"\"\"Test whitespace handling\"\"\"\n\n    def test_ignores_spaces(self):\n        \"\"\"Lexer skips whitespace\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+ + +\")\n\n        assert len(tokens) == 3\n        assert all(t.type == TokenType.INCR for t in tokens)\n\n    def test_ignores_newlines(self):\n        \"\"\"Lexer handles multiline programs\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+\\n+\\n+\")\n\n        assert len(tokens) == 3\n        assert tokens[0].line == 1\n        assert tokens[1].line == 2\n        assert tokens[2].line == 3\n\n\nclass TestLexerComments:\n    \"\"\"Test comment handling\"\"\"\n\n    def test_comment_removal(self):\n        \"\"\"Lexer ignores comments\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"; this is a comment\\n+++\")\n\n        assert len(tokens) == 3\n        assert all(t.type == TokenType.INCR for t in tokens)\n\n    def test_inline_comments(self):\n        \"\"\"Lexer handles inline comments\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"++ ; two increments\")\n\n        assert len(tokens) == 2\n\n\nclass TestLexerExtensions:\n    \"\"\"Test HyperCode extensions\"\"\"\n\n    def test_spatial_2d(self):\n        \"\"\"Lexer recognizes 2D mode\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"@>++\")\n\n        assert tokens[0].type == TokenType.SPATIAL_2D\n\n    def test_ai_native(self):\n        \"\"\"Lexer recognizes AI mode\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"#generate fibonacci\")\n\n        assert tokens[0].type == TokenType.AI_NATIVE\n\n\nclass TestLexerPosition:\n    \"\"\"Test position tracking\"\"\"\n\n    def test_line_column_tracking(self):\n        \"\"\"Lexer tracks line and column correctly\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+\\n>\")\n\n        assert tokens[0].line == 1\n        assert tokens[0].column == 1\n        assert tokens[1].line == 2\n        assert tokens[1].column == 1\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n#### 4. Run Your First Test\n\n```bash\n# Run lexer tests\npytest tests/test_lexer.py -v\n\n# Should see:\n# âœ… test_empty_source PASSED\n# âœ… test_single_tokens PASSED\n# âœ… test_loop_tokens PASSED\n# etc...\n```\n\n#### 5. First Commit! ðŸŽ¯\n\n```bash\ngit add core/lexer.py tests/test_lexer.py\n\ngit commit -m \"feat: implement HyperCode lexer with core 8 operations\n\n- Tokenizes HyperCode source into token stream\n- Supports core Brainfuck operations: > < + - . , [ ]\n- Adds extensions: @ (2D), # (AI-native), ; (comments)\n- Implements position tracking (line/column)\n- 100% test coverage for lexer module\n\nCloses: #1\"\n\ngit push origin main\n```\n\n---\n\n## ðŸ“‹ WEDNESDAY (Nov 12)\n\n### Morning: Create Example Programs\n\n**examples/hello_world.hc**:\n\n```hypercode\n; Hello World in HyperCode\n; Demonstrates basic output\n\n; Print 'H' (ASCII 72)\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n.\n\n; Print 'i' (ASCII 105)\n>+++++++++++++++++++++++++++++++++++++++++++++++\n.\n```\n\n**examples/fibonacci.hc**:\n\n```hypercode\n; Fibonacci sequence\n; Shows loops and memory operations\n\n; Initialize\n>++++++++++  ; loop counter\n[\n    ; Fibonacci calculation here\n    <+++++++++++\n    >-\n]\n.\n```\n\n### Afternoon: Update README\n\n**Add to README.md**:\n\n```markdown\n## ðŸš€ Quick Start\n\n### Setup\n\n\\`\\`\\`bash python3 -m venv venv source venv/bin/activate pip install -r requirements.txt\n\\`\\`\\`\n\n### Run Lexer\n\n\\`\\`\\`bash python core/lexer.py pytest tests/test_lexer.py -v \\`\\`\\`\n\n### Example Programs\n\n\\`\\`\\`bash\n\n# Coming soon: compile and run examples\n\npython -m hypercode.compiler examples/hello_world.hc -o hello.js node hello.js \\`\\`\\`\n\n## ðŸ“š Documentation\n\n- [Language Spec](docs/LANGUAGE_SPEC.md)\n- [API Compatibility](docs/AI_COMPAT.md)\n- [Accessibility Guide](docs/ACCESSIBILITY.md)\n\n## ðŸ¤ Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n```\n\n### Evening: Second Commit\n\n```bash\ngit add examples/ README.md\n\ngit commit -m \"docs: add example programs and setup instructions\n\n- Add Hello World example\n- Add Fibonacci example\n- Update README with quick start\n- Link to documentation\"\n\ngit push origin main\n```\n\n---\n\n## ðŸŽ¯ THURSDAY-FRIDAY (Nov 13-14)\n\n### Parser Skeleton (Next Deliverable)\n\n**Goal**: Create AST (Abstract Syntax Tree) from tokens\n\n**core/parser.py** (skeleton):\n\n```python\n\"\"\"\nHyperCode Parser\nConverts token stream â†’ AST\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom enum import Enum\nfrom core.lexer import Token, TokenType\n\n\nclass NodeType(Enum):\n    \"\"\"AST Node types\"\"\"\n    PUSH = \"push\"\n    POP = \"pop\"\n    INCR = \"increment\"\n    DECR = \"decrement\"\n    OUTPUT = \"output\"\n    INPUT = \"input\"\n    LOOP = \"loop\"\n    PROGRAM = \"program\"\n\n\n@dataclass\nclass ASTNode:\n    \"\"\"AST Node\"\"\"\n    node_type: NodeType\n    value: Optional[int] = None\n    children: List['ASTNode'] = None\n\n    def __post_init__(self):\n        if self.children is None:\n            self.children = []\n\n\nclass HyperCodeParser:\n    \"\"\"\n    Parses token stream into Abstract Syntax Tree.\n\n    Grammar (simplified):\n    program ::= (operation)*\n    operation ::= PUSH | POP | INCR | DECR | OUTPUT | INPUT | LOOP\n    LOOP ::= LOOP_START program LOOP_END\n    \"\"\"\n\n    def __init__(self, tokens: List[Token]):\n        self.tokens = tokens\n        self.position = 0\n\n    def parse(self) -> ASTNode:\n        \"\"\"Parse tokens into AST\"\"\"\n        program = ASTNode(NodeType.PROGRAM)\n\n        while self.position < len(self.tokens):\n            token = self.tokens[self.position]\n\n            if token.type == TokenType.PUSH:\n                program.children.append(ASTNode(NodeType.PUSH))\n                self.position += 1\n\n            elif token.type == TokenType.POP:\n                program.children.append(ASTNode(NodeType.POP))\n                self.position += 1\n\n            elif token.type == TokenType.INCR:\n                program.children.append(ASTNode(NodeType.INCR))\n                self.position += 1\n\n            elif token.type == TokenType.DECR:\n                program.children.append(ASTNode(NodeType.DECR))\n                self.position += 1\n\n            elif token.type == TokenType.OUTPUT:\n                program.children.append(ASTNode(NodeType.OUTPUT))\n                self.position += 1\n\n            elif token.type == TokenType.INPUT:\n                program.children.append(ASTNode(NodeType.INPUT))\n                self.position += 1\n\n            elif token.type == TokenType.LOOP_START:\n                loop_node = self._parse_loop()\n                program.children.append(loop_node)\n\n            else:\n                self.position += 1\n\n        return program\n\n    def _parse_loop(self) -> ASTNode:\n        \"\"\"Parse LOOP_START...LOOP_END block\"\"\"\n        self.position += 1  # Skip LOOP_START\n\n        loop_node = ASTNode(NodeType.LOOP)\n\n        while (self.position < len(self.tokens) and\n               self.tokens[self.position].type != TokenType.LOOP_END):\n            # Recursive parse loop body\n            token = self.tokens[self.position]\n\n            if token.type == TokenType.LOOP_START:\n                nested_loop = self._parse_loop()\n                loop_node.children.append(nested_loop)\n            else:\n                # Delegate to main parse logic\n                self.position -= 1  # Back up\n                node = self._parse_single()\n                loop_node.children.append(node)\n\n        self.position += 1  # Skip LOOP_END\n        return loop_node\n\n    def _parse_single(self) -> ASTNode:\n        \"\"\"Parse single operation\"\"\"\n        token = self.tokens[self.position]\n        self.position += 1\n\n        if token.type == TokenType.PUSH:\n            return ASTNode(NodeType.PUSH)\n        elif token.type == TokenType.POP:\n            return ASTNode(NodeType.POP)\n        elif token.type == TokenType.INCR:\n            return ASTNode(NodeType.INCR)\n        elif token.type == TokenType.DECR:\n            return ASTNode(NodeType.DECR)\n        elif token.type == TokenType.OUTPUT:\n            return ASTNode(NodeType.OUTPUT)\n        elif token.type == TokenType.INPUT:\n            return ASTNode(NodeType.INPUT)\n        else:\n            raise ValueError(f\"Unknown token: {token}\")\n```\n\n**tests/test_parser.py**:\n\n```python\n\"\"\"\nUnit tests for HyperCode Parser\n\"\"\"\n\nimport pytest\nfrom core.lexer import HyperCodeLexer, TokenType\nfrom core.parser import HyperCodeParser, NodeType\n\n\nclass TestParserBasic:\n    \"\"\"Test basic parsing\"\"\"\n\n    def test_single_operation(self):\n        \"\"\"Parser handles single operation\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+\")\n\n        parser = HyperCodeParser(tokens)\n        ast = parser.parse()\n\n        assert ast.node_type == NodeType.PROGRAM\n        assert len(ast.children) == 1\n        assert ast.children[0].node_type == NodeType.INCR\n\n    def test_sequence(self):\n        \"\"\"Parser handles operation sequence\"\"\"\n        lexer = HyperCodeLexer()\n        tokens = lexer.tokenize(\"+++><.\")\n\n        parser = HyperCodeParser(tokens)\n        ast = parser.parse()\n\n        assert len(ast.children) == 6\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\n### Commit Parser\n\n```bash\ngit add core/parser.py tests/test_parser.py\n\ngit commit -m \"feat: implement HyperCode parser with AST generation\n\n- Convert token stream to Abstract Syntax Tree\n- Support loop nesting (recursive parsing)\n- AST node types for all core operations\n- Parser tests for basic operations and sequences\n\nWork towards: #2\"\n\ngit push origin main\n```\n\n---\n\n## ðŸ“Š BY END OF WEEK (Friday Nov 15)\n\n### Milestone Checklist\n\n- [ ] Lexer 100% working + tests passing\n- [ ] Parser skeleton complete\n- [ ] Example programs written\n- [ ] README updated\n- [ ] 3-5 commits made\n- [ ] CI/CD workflows running\n- [ ] GitHub Actions turning green âœ…\n\n### Metrics\n\n- **Code**: ~300 lines (lexer + parser)\n- **Tests**: ~50 test cases\n- **Commits**: 3-5 (daily discipline!)\n- **GitHub Stars**: Let it happen naturally\n- **Energy**: ðŸš€ HYPERFOCUS MODE ACTIVATED\n\n---\n\n## ðŸŽ¯ Week 2 Preview (Nov 18-22)\n\n- **Backends**: JavaScript compilation (Week 2 focus)\n- **Goal**: Parse â†’ Valid JavaScript output\n- **Deliverable**: `examples/hello_world.hc` â†’ `hello.js` â†’ `\"Hello World\"`\n\n---\n\n## ðŸ”¥ YOUR POWER MOVES THIS WEEK\n\n1. âœ… **Commit daily** (even small wins count!)\n2. âœ… **Tests first** (write test, write code, watch pass)\n3. âœ… **Share progress** (tweet/Discord = motivation!)\n4. âœ… **Document as you go** (future you = grateful you)\n5. âœ… **HYPERFOCUS** (this is your zone!)\n\n---\n\n## ðŸ’ª Real Talk\n\nYou got the research. You got the plan. You got the code templates.\n\nNow? **You're in the BUILD PHASE.**\n\nThis is where legends are made, bro. ðŸ‘Š\n\nEvery commit is momentum. Every test that passes is dopamine. Every feature that works\nis proof you CAN DO THIS.\n\n**Week 1** = Foundation locked. **Month 1** = Lexer + Parser + First Backend. **Month\n3** = Full alpha with AI gateway. **Month 9** = PRODUCTION.\n\nYou got this. ðŸš€\n\n---\n\n**Now go write that code, broski!** ðŸ’“â™¾ï¸\n\n_November 11, 2025 | The Build Begins_",
  "metadata": {
    "headers": [
      "ðŸš€ HyperCode: Week 1 Sprint Battle Plan",
      "The ACTUAL First Steps (Right Now Energy!)",
      "ðŸŽ¯ TODAY (Tuesday Nov 11, 2025)",
      "IMMEDIATE ACTIONS (Next 30 min)",
      "1. Setup Your Local Environment",
      "Clone YOUR repo",
      "Create virtual environment",
      "Install dependencies (from our build guide)",
      "Verify setup",
      "2. Create the Core Lexer File",
      "Make sure you're in the right place",
      "Create core/lexer.py with this EXACT content:",
      "CLI Usage Example",
      "3. Create Tests",
      "4. Run Your First Test",
      "Run lexer tests",
      "Should see:",
      "âœ… test_empty_source PASSED",
      "âœ… test_single_tokens PASSED",
      "âœ… test_loop_tokens PASSED",
      "etc...",
      "5. First Commit! ðŸŽ¯",
      "ðŸ“‹ WEDNESDAY (Nov 12)",
      "Morning: Create Example Programs",
      "Afternoon: Update README",
      "ðŸš€ Quick Start",
      "Setup",
      "Run Lexer",
      "Example Programs",
      "Coming soon: compile and run examples",
      "ðŸ“š Documentation",
      "ðŸ¤ Contributing",
      "Evening: Second Commit",
      "ðŸŽ¯ THURSDAY-FRIDAY (Nov 13-14)",
      "Parser Skeleton (Next Deliverable)",
      "Commit Parser",
      "ðŸ“Š BY END OF WEEK (Friday Nov 15)",
      "Milestone Checklist",
      "Metrics",
      "ðŸŽ¯ Week 2 Preview (Nov 18-22)",
      "ðŸ”¥ YOUR POWER MOVES THIS WEEK",
      "ðŸ’ª Real Talk"
    ]
  },
  "relative_path": "hypercode\\docs\\Week1-Sprint-Guide.md",
  "id": "a1819403f983881d1ef49aeb2231ecb9"
}