{
  "file_name": "hypercode-lexer-COMPLETE.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\src\\hypercode-lexer-COMPLETE.py",
  "file_size": 9437,
  "created": "2025-11-15T15:54:41.052530",
  "modified": "2025-11-15T15:54:41.052530",
  "file_type": "code",
  "content_hash": "d58867a703c9c4b16f371b9ac62dbb10",
  "content_type": "text",
  "content": "\"\"\"\nHyperCode Lexer - Complete Implementation\nTokenizes HyperCode programs with full neurodivergent-friendly features\n\nFeatures:\n- 8 core operations (Brainfuck-inspired)\n- Spatial 2D mode (@)\n- AI-native markers (#)\n- Comments (;)\n- Position tracking\n- Error reporting\n- Colorized output (for ADHD/dyslexia)\n\"\"\"\n\nimport sys\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import List, Optional\n\n\nclass TokenType(Enum):\n    \"\"\"HyperCode token types - minimal yet powerful\"\"\"\n\n    # Core operations (Brainfuck-inspired)\n    PUSH = \"PUSH\"  # > move pointer right\n    POP = \"POP\"  # < move pointer left\n    INCR = \"INCR\"  # + increment cell\n    DECR = \"DECR\"  # - decrement cell\n    OUTPUT = \"OUTPUT\"  # . output character\n    INPUT = \"INPUT\"  # , read character\n    LOOP_START = \"LOOP_START\"  # [ start loop\n    LOOP_END = \"LOOP_END\"  # ] end loop\n\n    # HyperCode extensions\n    SPATIAL_2D = \"SPATIAL_2D\"  # @ enter 2D spatial mode\n    AI_NATIVE = \"AI_NATIVE\"  # # AI-native code marker\n    COMMENT = \"COMMENT\"  # ; comment line\n\n    # Special\n    EOF = \"EOF\"  # End of file\n    UNKNOWN = \"UNKNOWN\"  # Unknown character\n\n\n@dataclass\nclass Token:\n    \"\"\"Represents a single token with position tracking\"\"\"\n\n    type: TokenType\n    value: str\n    position: int\n    line: int\n    column: int\n\n    def __repr__(self) -> str:\n        \"\"\"Neurodivergent-friendly representation\"\"\"\n        return f\"Token({self.type.value:15} | '{self.value}' | L{self.line}:C{self.column})\"\n\n\nclass LexerError(Exception):\n    \"\"\"Lexer-specific errors with context\"\"\"\n\n    def __init__(self, message: str, line: int, column: int):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(f\"Line {line}, Col {column}: {message}\")\n\n\nclass HyperCodeLexer:\n    \"\"\"\n    Tokenizes HyperCode programs with accessibility features.\n\n    Design principles:\n    - Clear error messages (neurodivergent-friendly)\n    - Position tracking (helps with debugging)\n    - Colorized output (optional, for visual learners)\n    - Comments preserved (for documentation)\n    \"\"\"\n\n    # Token mapping (simple, explicit)\n    TOKEN_MAP = {\n        \">\": TokenType.PUSH,\n        \"<\": TokenType.POP,\n        \"+\": TokenType.INCR,\n        \"-\": TokenType.DECR,\n        \".\": TokenType.OUTPUT,\n        \",\": TokenType.INPUT,\n        \"[\": TokenType.LOOP_START,\n        \"]\": TokenType.LOOP_END,\n        \"@\": TokenType.SPATIAL_2D,\n        \"#\": TokenType.AI_NATIVE,\n        \";\": TokenType.COMMENT,\n    }\n\n    def __init__(self, source: str, filename: str = \"<stdin>\"):\n        \"\"\"\n        Initialize lexer with source code.\n\n        Args:\n            source: HyperCode source code\n            filename: Source filename (for error reporting)\n        \"\"\"\n        self.source = source\n        self.filename = filename\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n\n    def tokenize(self) -> List[Token]:\n        \"\"\"\n        Convert HyperCode source to token stream.\n\n        Returns:\n            List of Token objects\n\n        Raises:\n            LexerError: On invalid syntax\n        \"\"\"\n        self.tokens = []\n\n        while self.position < len(self.source):\n            char = self.source[self.position]\n\n            # Handle comments (skip until end of line)\n            if char == \";\":\n                self._skip_comment()\n                continue\n\n            # Handle whitespace (track position but don't create tokens)\n            if char.isspace():\n                self._advance_position(char)\n                continue\n\n            # Handle valid tokens\n            if char in self.TOKEN_MAP:\n                token_type = self.TOKEN_MAP[char]\n                token = Token(\n                    type=token_type,\n                    value=char,\n                    position=self.position,\n                    line=self.line,\n                    column=self.column,\n                )\n                self.tokens.append(token)\n                self._advance_position(char)\n            else:\n                # Unknown character - helpful error message\n                raise LexerError(\n                    f\"Unexpected character: '{char}' (ASCII {ord(char)})\\n\"\n                    f\"Valid operations: > < + - . , [ ] @ # ;\",\n                    self.line,\n                    self.column,\n                )\n\n        # Add EOF token\n        self.tokens.append(\n            Token(\n                type=TokenType.EOF,\n                value=\"\",\n                position=self.position,\n                line=self.line,\n                column=self.column,\n            )\n        )\n\n        return self.tokens\n\n    def _advance_position(self, char: str):\n        \"\"\"Update position tracking after processing character\"\"\"\n        self.position += 1\n\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n\n    def _skip_comment(self):\n        \"\"\"Skip characters until end of line\"\"\"\n        while self.position < len(self.source) and self.source[self.position] != \"\\n\":\n            self._advance_position(self.source[self.position])\n\n    def get_tokens(self) -> List[Token]:\n        \"\"\"Return current token list\"\"\"\n        return self.tokens\n\n    def filter_tokens(\n        self, exclude_types: Optional[List[TokenType]] = None\n    ) -> List[Token]:\n        \"\"\"\n        Get tokens excluding certain types.\n\n        Args:\n            exclude_types: Token types to exclude\n\n        Returns:\n            Filtered token list\n        \"\"\"\n        if exclude_types is None:\n            exclude_types = [TokenType.UNKNOWN, TokenType.EOF]\n\n        return [t for t in self.tokens if t.type not in exclude_types]\n\n    def print_tokens(self, colorize: bool = False):\n        \"\"\"\n        Print tokens in readable format.\n\n        Args:\n            colorize: Use ANSI colors (helps ADHD/dyslexia)\n        \"\"\"\n        if colorize and sys.stdout.isatty():\n            # Color codes for different token types\n            colors = {\n                TokenType.PUSH: \"\\033[94m\",  # Blue\n                TokenType.POP: \"\\033[94m\",  # Blue\n                TokenType.INCR: \"\\033[92m\",  # Green\n                TokenType.DECR: \"\\033[92m\",  # Green\n                TokenType.OUTPUT: \"\\033[93m\",  # Yellow\n                TokenType.INPUT: \"\\033[93m\",  # Yellow\n                TokenType.LOOP_START: \"\\033[95m\",  # Magenta\n                TokenType.LOOP_END: \"\\033[95m\",  # Magenta\n                TokenType.SPATIAL_2D: \"\\033[96m\",  # Cyan\n                TokenType.AI_NATIVE: \"\\033[91m\",  # Red\n            }\n            reset = \"\\033[0m\"\n\n            for token in self.tokens:\n                color = colors.get(token.type, \"\")\n                print(f\"{color}{token}{reset}\")\n        else:\n            # Plain text output\n            for token in self.tokens:\n                print(token)\n\n    def get_statistics(self) -> dict:\n        \"\"\"\n        Get token statistics (useful for analysis).\n\n        Returns:\n            Dictionary with token counts\n        \"\"\"\n        stats = {}\n        for token in self.tokens:\n            if token.type != TokenType.EOF:\n                stats[token.type.value] = stats.get(token.type.value, 0) + 1\n        return stats\n\n\ndef main():\n    \"\"\"CLI interface for the lexer\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"HyperCode Lexer - Tokenize HyperCode programs\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python lexer.py program.hc\n  python lexer.py program.hc --colorize\n  echo \"+++>.\" | python lexer.py -\n        \"\"\",\n    )\n\n    parser.add_argument(\"file\", help=\"HyperCode source file (or - for stdin)\")\n    parser.add_argument(\n        \"--colorize\",\n        action=\"store_true\",\n        help=\"Use colored output (ADHD/dyslexia-friendly)\",\n    )\n    parser.add_argument(\"--stats\", action=\"store_true\", help=\"Show token statistics\")\n\n    args = parser.parse_args()\n\n    # Read source\n    if args.file == \"-\":\n        source = sys.stdin.read()\n        filename = \"<stdin>\"\n    else:\n        try:\n            with open(args.file, \"r\") as f:\n                source = f.read()\n            filename = args.file\n        except FileNotFoundError:\n            print(f\"‚ùå Error: File not found: {args.file}\", file=sys.stderr)\n            return 1\n\n    # Tokenize\n    try:\n        lexer = HyperCodeLexer(source, filename)\n        tokens = lexer.tokenize()\n\n        print(f\"‚úÖ Successfully tokenized: {filename}\")\n        print(f\"   Total tokens: {len(tokens) - 1}\")  # Exclude EOF\n        print()\n\n        # Print tokens\n        lexer.print_tokens(colorize=args.colorize)\n\n        # Print statistics\n        if args.stats:\n            print(\"\\nüìä Token Statistics:\")\n            stats = lexer.get_statistics()\n            for token_type, count in sorted(stats.items()):\n                print(f\"   {token_type:15} : {count}\")\n\n        return 0\n\n    except LexerError as e:\n        print(f\"‚ùå Lexer Error: {e}\", file=sys.stderr)\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\src\\hypercode-lexer-COMPLETE.py",
  "id": "92bb4558f154d28cbf0b644977d971b1"
}