{
  "file_name": "lexer.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\src\\hypercode\\core\\lexer.py",
  "file_size": 10577,
  "created": "2025-11-15T15:54:41.061463",
  "modified": "2025-11-26T02:00:12.352607",
  "file_type": "code",
  "content_hash": "ef186e9dc86d8c6ef717aecf86c70d21",
  "content_type": "text",
  "content": "# Copyright 2025 welshDog (Lyndz Williams)\n#\n# Licensed under the MIT License (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://opensource.org/licenses/MIT\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCore HyperCode language implementation - Lexer\n\nThis module implements the lexical analyzer for the HyperCode language.\nIt converts source code into a sequence of tokens for further processing.\n\"\"\"\n\nimport re\nfrom typing import List, Optional, Pattern, Tuple, Union\n\nfrom .tokens import Token as BaseToken\nfrom .tokens import TokenType\n\n# The Token class is now imported from tokens.py, but we'll keep the same interface\nToken = BaseToken\n\n\nclass LexerError(Exception):\n    \"\"\"Exception raised for errors in the lexer.\"\"\"\n\n    def __init__(self, message: str, line: int, column: int):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(f\"{message} at line {line}, column {column}\")\n\n\nclass Lexer:\n    \"\"\"\n    Lexical analyzer for the HyperCode language.\n\n    Converts source code into a sequence of tokens that can be processed\n    by the parser.\n    \"\"\"\n\n    # Define keywords mapping\n    KEYWORDS = {\n        \"if\": TokenType.IF,\n        \"else\": TokenType.ELSE,\n        \"for\": TokenType.FOR,\n        \"while\": TokenType.WHILE,\n        \"function\": TokenType.FUN,  # Changed from FUNCTION to FUN to match tokens.py\n        \"return\": TokenType.RETURN,\n        \"let\": TokenType.LET,\n        \"const\": TokenType.CONST,\n        \"var\": TokenType.VAR,\n        \"true\": TokenType.TRUE,\n        \"false\": TokenType.FALSE,\n        \"null\": TokenType.NIL,  # Changed from NULL to NIL to match tokens.py\n        \"print\": TokenType.PRINT,\n        \"intent\": TokenType.INTENT,\n    }\n\n    # Token specifications - ordered by priority (longer patterns first)\n    TOKENS = [\n        # Skip whitespace and comments (they won't be included in the token stream)\n        (None, r\"\\s+\"),  # Whitespace\n        (None, r\"#[^\\r\\n]*|//[^\\r\\n]*|/\\*[\\s\\S]*?\\*/\"),  # Comments\n        # Multi-character operators (must come before single-character ones)\n        (TokenType.EQUAL_EQUAL, r\"==\"),\n        (TokenType.BANG_EQUAL, r\"!=\"),\n        (TokenType.LESS_EQUAL, r\"<=\"),\n        (TokenType.GREATER_EQUAL, r\">=\"),\n        # Single-character operators\n        (TokenType.PLUS, r\"\\+\"),\n        (TokenType.MINUS, r\"-\"),\n        (TokenType.STAR, r\"\\*\"),  # Handles both * and ** (handled in parser)\n        (TokenType.SLASH, r\"/\"),\n        (TokenType.EQUAL, r\"=\"),\n        (TokenType.BANG, r\"!\"),\n        (TokenType.LESS, r\"<\"),\n        (TokenType.GREATER, r\">\"),\n        # String literals (handle both single and double quoted strings with escaped quotes)\n        (TokenType.STRING, r'\"(?:[^\"\\\\]|\\\\.)*\"|\\'(?:[^\\'\\\\]|\\\\.)*\\''),\n        # Numbers (handle integers, decimals, and scientific notation)\n        (\n            TokenType.NUMBER,\n            r\"\\d+(?:_?\\d+)*\\.\\d+([eE][+-]?\\d+)?|\\d+(?:_?\\d+)*[eE][+-]?\\d+|\\.\\d+([eE][+-]?\\d+)?|0[xX][0-9a-fA-F_]+|0[bB][01_]+|0[oO]?[0-7_]+|\\d+(?:_?\\d+)*\",\n        ),\n        # Punctuation\n        (TokenType.LPAREN, r\"\\(\"),\n        (TokenType.RPAREN, r\"\\)\"),\n        (TokenType.LBRACE, r\"\\{\"),\n        (TokenType.RBRACE, r\"\\}\"),\n        (TokenType.LBRACKET, r\"\\[\"),\n        (TokenType.RBRACKET, r\"\\]\"),\n        (TokenType.COMMA, r\",\"),\n        (TokenType.SEMICOLON, r\";\"),\n        (TokenType.COLON, r\":\"),\n        (TokenType.DOT, r\"\\.\"),\n        # Keywords and identifiers (must come after operators to avoid conflicts)\n        (TokenType.IDENTIFIER, r\"[a-zA-Z_]\\w*\"),\n    ]\n\n    def __init__(self, source: str = \"\"):\n        \"\"\"Initialize the lexer with source code.\n\n        Args:\n            source: The source code to tokenize\n        \"\"\"\n        self.tokens: List[Token] = []\n        self.pos = 0\n        self.line = 1\n        self.column = 1\n        self.source = source\n\n        # Compile regex patterns\n        self.token_patterns: List[Tuple[Optional[TokenType], Pattern[str]]] = []\n        for token_type, pattern in self.TOKENS:\n            self.token_patterns.append((token_type, re.compile(pattern)))\n\n    def tokenize(self, source: Optional[str] = None) -> List[Token]:\n        \"\"\"\n        Convert source code into a list of tokens.\n\n        Args:\n            source: Optional source code to tokenize. If not provided, uses the source\n                   passed to the constructor.\n\n        Returns:\n            List of Token objects\n\n        Raises:\n            ValueError: If no source code is provided\n        \"\"\"\n        if source is not None:\n            self.source = source\n\n        if not self.source:\n            raise ValueError(\"No source code provided to tokenize\")\n\n        # Reset state\n        self.pos = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n\n        while self.pos < len(self.source):\n            if not self._match_patterns():\n                # Handle unknown characters\n                self._handle_unknown()\n\n        # Add EOF token\n        self._add_token(TokenType.EOF, \"\")\n        return self.tokens\n\n    def _match_patterns(self) -> bool:\n        \"\"\"Try to match the current position against all token patterns.\"\"\"\n        for token_type, pattern in self.token_patterns:\n            match = pattern.match(self.source, self.pos)\n            if match:\n                value = match.group(0)\n\n                # Update position first for accurate line/column tracking\n                self._update_position(value)\n\n                # Skip tokens with None type (whitespace and comments)\n                if token_type is None:\n                    self.pos = match.end()\n                    return True\n\n                # Special handling for identifiers that might be keywords\n                if token_type == TokenType.IDENTIFIER and value in self.KEYWORDS:\n                    token_type = self.KEYWORDS[value]\n\n                # Add the token\n                self._add_token(token_type, value)\n                self.pos = match.end()\n                return True\n\n        return False\n\n    def _update_position(self, text: str) -> None:\n        \"\"\"Update line and column numbers based on the given text.\"\"\"\n        lines = text.splitlines(keepends=True)\n        if not lines:\n            return\n\n        # Update line and column based on the last line\n        last_line = lines[-1]\n        if \"\\n\" in last_line or \"\\r\" in last_line:\n            # If the last character is a newline, move to the start of the next line\n            self.line += len(lines)\n            self.column = 1\n        else:\n            # Otherwise, just update the column\n            self.column += len(last_line)\n\n        # Add any additional newlines from the text\n        self.line += text.count(\"\\n\")\n\n    def _add_token(self, token_type: TokenType, lexeme: str) -> None:\n        \"\"\"Add a token to the token list.\n\n        Args:\n            token_type: The type of the token.\n            lexeme: The lexeme of the token (the actual text from the source).\n        \"\"\"\n        if token_type is not None:\n            # For literals, we need to set the appropriate value\n            literal: Optional[Union[int, float, str, bool, None]] = None\n            if token_type == TokenType.NUMBER:\n                try:\n                    if lexeme.lower().startswith(\"0x\"):\n                        # Parse as hexadecimal\n                        literal = int(lexeme, 16)\n                    elif lexeme.lower().startswith(\"0b\"):\n                        # Parse as binary\n                        literal = int(lexeme[2:], 2)\n                    elif lexeme.lower().startswith(\"0o\"):\n                        # Parse as octal\n                        literal = int(lexeme[2:], 8)\n                    else:\n                        # Try to parse as int first, then as float\n                        try:\n                            # Handle underscores in numbers (e.g., 1_000_000)\n                            clean_lexeme = lexeme.replace(\"_\", \"\")\n                            literal = int(clean_lexeme)\n                        except ValueError:\n                            try:\n                                literal = float(clean_lexeme)\n                            except ValueError:\n                                # If we can't parse it as a number, keep literal as None\n                                pass\n                except Exception:\n                    # If there's any error in parsing, keep literal as None\n                    pass\n            elif token_type == TokenType.STRING:\n                # Remove the quotes from the string literal\n                literal = lexeme[1:-1]\n                # Handle escape sequences\n                try:\n                    # Use a simple approach to handle common escape sequences\n                    literal = literal.encode().decode(\"unicode_escape\")\n                except UnicodeDecodeError:\n                    # If there's an error in unescaping, keep the original\n                    pass\n            elif token_type in (TokenType.TRUE, TokenType.FALSE):\n                literal = token_type == TokenType.TRUE\n            elif token_type == TokenType.NIL:\n                literal = None\n\n            self.tokens.append(\n                Token(\n                    type=token_type,\n                    lexeme=lexeme,\n                    literal=literal,\n                    line=self.line,\n                    column=self.column - len(lexeme),\n                )\n            )\n\n            # For debugging\n            # print(f\"Added token: {self.tokens[-1]}\")\n\n    def _handle_unknown(self) -> None:\n        \"\"\"Handle unknown characters in the source.\"\"\"\n        char = self.source[self.pos]\n        if char.strip():  # Only raise for non-whitespace characters\n            raise SyntaxError(\n                f\"Invalid character '{char}' at line {self.line}, column {self.column}\"\n            )\n        # Skip the unknown character\n        self.pos += 1\n        self.column += 1\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\src\\hypercode\\core\\lexer.py",
  "id": "50eef896d2a14139c59dba81a58a3b59"
}