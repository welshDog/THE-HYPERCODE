{
  "file_name": "final_integration_test.py",
  "file_path": "C:\\Users\\lyndz\\Downloads\\hypercode PROJECT\\hypercode\\src\\core\\hypercode-\\final_integration_test.py",
  "file_size": 4353,
  "created": "2025-11-18T08:17:21.928477",
  "modified": "2025-11-18T19:47:10.997708",
  "file_type": "code",
  "content_hash": "081ba0cb2ce962fe8d209ee569c79fee",
  "content_type": "text",
  "content": "#!/usr/bin/env python3\n\"\"\"\nFinal Test: Complete Perplexity Space Integration\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n# Add src to path\nsys.path.insert(0, str(Path(__file__).parent / \"src\"))\n\nfrom hypercode.enhanced_perplexity_client import EnhancedPerplexityClient\n\n\ndef final_integration_test():\n    \"\"\"Complete test of the Perplexity Space integration\"\"\"\n    client = EnhancedPerplexityClient()\n\n    print(\"ðŸŽ‰ FINAL INTEGRATION TEST\")\n    print(\"=\" * 50)\n    print(\"Testing your Perplexity API with implementation guide knowledge...\")\n\n    # Test scenarios that should now work\n    test_scenarios = [\n        {\n            \"query\": \"What are the eight pillars of HyperCode?\",\n            \"expected_keywords\": [\"pillars\", \"visual-first\", \"neurodiversity\"],\n            \"description\": \"8 Pillars Knowledge\",\n        },\n        {\n            \"query\": \"How do I audit neurodiversity features?\",\n            \"expected_keywords\": [\"audit\", \"checklist\", \"neurodiversity\"],\n            \"description\": \"Audit Framework Knowledge\",\n        },\n        {\n            \"query\": \"What are the implementation phases?\",\n            \"expected_keywords\": [\"phases\", \"foundation\", \"expansion\", \"maturity\"],\n            \"description\": \"Implementation Roadmap Knowledge\",\n        },\n        {\n            \"query\": \"What metrics should I track for ADHD optimization?\",\n            \"expected_keywords\": [\"ADHD\", \"metrics\", \"optimization\"],\n            \"description\": \"ADHD Metrics Knowledge\",\n        },\n    ]\n\n    success_count = 0\n\n    for i, scenario in enumerate(test_scenarios, 1):\n        print(f'\\nðŸ§ª Test {i}: {scenario[\"description\"]}')\n        print(f'â“ Query: {scenario[\"query\"]}')\n\n        # Get context\n        context = client.knowledge_base.get_context_for_query(scenario[\"query\"])\n        print(f\"ðŸ“ Context: {len(context)} characters\")\n\n        # Query API\n        response = client.query_with_context(scenario[\"query\"], use_knowledge_base=True)\n\n        if \"choices\" in response and response[\"choices\"]:\n            content = response[\"choices\"][0][\"message\"][\"content\"]\n            print(f\"âœ… Response: {len(content)} characters\")\n\n            # Check for expected keywords\n            content_lower = content.lower()\n            found_keywords = [\n                kw\n                for kw in scenario[\"expected_keywords\"]\n                if kw.lower() in content_lower\n            ]\n\n            keyword_match = len(found_keywords) >= 2  # At least 2 keywords\n\n            if keyword_match:\n                print(\n                    f'ðŸŽ¯ SUCCESS: Found {len(found_keywords)}/{len(scenario[\"expected_keywords\"])} keywords: {found_keywords}'\n                )\n                success_count += 1\n            else:\n                print(\n                    f'âš ï¸  PARTIAL: Found {len(found_keywords)}/{len(scenario[\"expected_keywords\"])} keywords: {found_keywords}'\n                )\n\n            # Show preview\n            preview = content.replace(\"\\n\", \" \")[:150]\n            print(f\"ðŸ“„ Preview: {preview}...\")\n        else:\n            print(\"âŒ FAILED: No response\")\n\n    # Summary\n    print(\"\\nðŸ“Š FINAL RESULTS:\")\n    print(f\"âœ… Passed: {success_count}/{len(test_scenarios)} tests\")\n    print(f\"ðŸ§  Knowledge Base: {len(client.list_research_documents())} documents\")\n    print(\"ðŸ” Search Algorithm: Enhanced with related terms\")\n    print(\"ðŸ’¾ Memory System: Persistent and functional\")\n\n    if success_count >= 3:\n        print(\"\\nðŸŽ‰ INTEGRATION SUCCESS!\")\n        print(\"Your Perplexity API now remembers your implementation guide!\")\n        print(\"You can ask about pillars, audits, phases, and metrics!\")\n    else:\n        print(\"\\nâš ï¸  Partial success - some queries need improvement\")\n\n    # Show what's available\n    print(\"\\nðŸ“š Available Knowledge:\")\n    docs = client.list_research_documents()\n    for doc in docs:\n        print(f\"  ðŸ“„ {doc.title}\")\n\n    return success_count >= 3\n\n\nif __name__ == \"__main__\":\n    success = final_integration_test()\n\n    if success:\n        print(\"\\nðŸš€ Your Perplexity Space integration is COMPLETE!\")\n        print(\"   Ready to use with your actual research data!\")\n    else:\n        print(\"\\nðŸ”§ Integration needs fine-tuning for optimal results\")\n",
  "metadata": {},
  "relative_path": "src\\core\\hypercode-\\final_integration_test.py",
  "id": "8c6479613024beefe5af3f2e1a2e7142"
}