# ðŸš€ HyperCode: Week 1 Sprint Battle Plan

## The ACTUAL First Steps (Right Now Energy!)

**Status**: YOUR REPO IS LIVE! ðŸ”¥ **Mission**: Get Week 1 LOCKED IN **Timeline**: This
week â†’ Production-ready lexer

---

## ðŸŽ¯ TODAY (Tuesday Nov 11, 2025)

### IMMEDIATE ACTIONS (Next 30 min)

#### 1. Setup Your Local Environment

```bash
# Clone YOUR repo
git clone https://github.com/welshDog/hypercode.git
cd hypercode

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies (from our build guide)
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Verify setup
python --version  # Should be 3.10+
pytest --version
```

#### 2. Create the Core Lexer File

```bash
# Make sure you're in the right place
ls -la core/  # Should exist from setup

# Create core/lexer.py with this EXACT content:
```

**core/lexer.py**:

```python
"""
HyperCode Lexer (Tokenizer)
Converts HyperCode source â†’ Token stream
"""

from dataclasses import dataclass
from typing import List, Tuple
from enum import Enum

class TokenType(Enum):
    """HyperCode token types - inspired by Brainfuck minimalism"""
    PUSH = "PUSH"           # > move pointer right
    POP = "POP"             # < move pointer left
    INCR = "INCR"           # + increment cell
    DECR = "DECR"           # - decrement cell
    OUTPUT = "OUTPUT"       # . output character
    INPUT = "INPUT"         # , read character
    LOOP_START = "LOOP_START"   # [ start loop
    LOOP_END = "LOOP_END"       # ] end loop
    SPATIAL_2D = "SPATIAL_2D"   # @ enter 2D mode (Befunge-style)
    AI_NATIVE = "AI_NATIVE"     # # AI-native mode marker
    COMMENT = "COMMENT"         # ; ignore rest of line
    UNKNOWN = "UNKNOWN"


@dataclass
class Token:
    """Represents a single token"""
    type: TokenType
    value: str
    position: int
    line: int
    column: int


class HyperCodeLexer:
    """
    Tokenizes HyperCode programs.

    Minimal 8-core operations + extensions:
    > < + - . , [ ]    (Brainfuck core)
    @                   (2D mode)
    #                   (AI-native)
    ;                   (comments)
    """

    TOKEN_MAP = {
        '>': TokenType.PUSH,
        '<': TokenType.POP,
        '+': TokenType.INCR,
        '-': TokenType.DECR,
        '.': TokenType.OUTPUT,
        ',': TokenType.INPUT,
        '[': TokenType.LOOP_START,
        ']': TokenType.LOOP_END,
        '@': TokenType.SPATIAL_2D,
        '#': TokenType.AI_NATIVE,
        ';': TokenType.COMMENT,
    }

    def __init__(self):
        self.source = ""
        self.position = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []

    def tokenize(self, source: str) -> List[Token]:
        """
        Convert HyperCode source to token stream.

        Args:
            source: Raw HyperCode program text

        Returns:
            List of Token objects
        """
        self.source = source
        self.position = 0
        self.line = 1
        self.column = 1
        self.tokens = []

        while self.position < len(self.source):
            char = self.source[self.position]

            # Handle comments
            if char == ';':
                self._skip_comment()
                continue

            # Handle whitespace
            if char.isspace():
                self._advance_position(char)
                continue

            # Handle tokens
            if char in self.TOKEN_MAP:
                token_type = self.TOKEN_MAP[char]
                token = Token(
                    type=token_type,
                    value=char,
                    position=self.position,
                    line=self.line,
                    column=self.column
                )
                self.tokens.append(token)
            else:
                # Unknown character - create UNKNOWN token or skip
                token = Token(
                    type=TokenType.UNKNOWN,
                    value=char,
                    position=self.position,
                    line=self.line,
                    column=self.column
                )
                self.tokens.append(token)

            self._advance_position(char)

        return self.tokens

    def _advance_position(self, char: str):
        """Update position tracking after processing character"""
        self.position += 1
        self.column += 1

        if char == '\n':
            self.line += 1
            self.column = 1

    def _skip_comment(self):
        """Skip until end of line"""
        while self.position < len(self.source) and self.source[self.position] != '\n':
            self._advance_position(self.source[self.position])

    def get_tokens(self) -> List[Token]:
        """Return current token list"""
        return self.tokens

    def filter_tokens(self, exclude_types: List[TokenType] = None) -> List[Token]:
        """
        Get tokens excluding certain types (e.g., UNKNOWN).

        Args:
            exclude_types: Token types to exclude

        Returns:
            Filtered token list
        """
        if exclude_types is None:
            exclude_types = [TokenType.UNKNOWN]

        return [t for t in self.tokens if t.type not in exclude_types]


# CLI Usage Example
if __name__ == "__main__":
    # Example HyperCode program: print "Hi"
    program = """
    ; Print Hello in HyperCode
    ++++++++  ; H = 72
    >++++++++++  ; i = 105
    <<.>.
    """

    lexer = HyperCodeLexer()
    tokens = lexer.tokenize(program)

    print(f"Found {len(tokens)} tokens:")
    for token in tokens:
        if token.type != TokenType.COMMENT:
            print(f"  {token.type.value:15} | {token.value!r:5} | L{token.line}:C{token.column}")
```

#### 3. Create Tests

**tests/test_lexer.py**:

```python
"""
Unit tests for HyperCode Lexer
"""

import pytest
from core.lexer import HyperCodeLexer, TokenType, Token


class TestLexerBasic:
    """Test basic tokenization"""

    def test_empty_source(self):
        """Lexer handles empty source"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("")
        assert len(tokens) == 0

    def test_single_tokens(self):
        """Lexer recognizes core 8 tokens"""
        lexer = HyperCodeLexer()

        token_chars = "><+-.,"
        for char in token_chars:
            tokens = lexer.tokenize(char)
            assert len(tokens) == 1
            assert tokens[0].value == char

    def test_loop_tokens(self):
        """Lexer recognizes loop syntax"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("[+]")

        assert len(tokens) == 3
        assert tokens[0].type == TokenType.LOOP_START
        assert tokens[1].type == TokenType.INCR
        assert tokens[2].type == TokenType.LOOP_END

    def test_multiple_operations(self):
        """Lexer handles sequences"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+++>.")

        assert len(tokens) == 5
        assert tokens[0].type == TokenType.INCR
        assert tokens[1].type == TokenType.INCR
        assert tokens[2].type == TokenType.INCR
        assert tokens[3].type == TokenType.PUSH
        assert tokens[4].type == TokenType.OUTPUT


class TestLexerWhitespace:
    """Test whitespace handling"""

    def test_ignores_spaces(self):
        """Lexer skips whitespace"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+ + +")

        assert len(tokens) == 3
        assert all(t.type == TokenType.INCR for t in tokens)

    def test_ignores_newlines(self):
        """Lexer handles multiline programs"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+\n+\n+")

        assert len(tokens) == 3
        assert tokens[0].line == 1
        assert tokens[1].line == 2
        assert tokens[2].line == 3


class TestLexerComments:
    """Test comment handling"""

    def test_comment_removal(self):
        """Lexer ignores comments"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("; this is a comment\n+++")

        assert len(tokens) == 3
        assert all(t.type == TokenType.INCR for t in tokens)

    def test_inline_comments(self):
        """Lexer handles inline comments"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("++ ; two increments")

        assert len(tokens) == 2


class TestLexerExtensions:
    """Test HyperCode extensions"""

    def test_spatial_2d(self):
        """Lexer recognizes 2D mode"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("@>++")

        assert tokens[0].type == TokenType.SPATIAL_2D

    def test_ai_native(self):
        """Lexer recognizes AI mode"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("#generate fibonacci")

        assert tokens[0].type == TokenType.AI_NATIVE


class TestLexerPosition:
    """Test position tracking"""

    def test_line_column_tracking(self):
        """Lexer tracks line and column correctly"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+\n>")

        assert tokens[0].line == 1
        assert tokens[0].column == 1
        assert tokens[1].line == 2
        assert tokens[1].column == 1


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

#### 4. Run Your First Test

```bash
# Run lexer tests
pytest tests/test_lexer.py -v

# Should see:
# âœ… test_empty_source PASSED
# âœ… test_single_tokens PASSED
# âœ… test_loop_tokens PASSED
# etc...
```

#### 5. First Commit! ðŸŽ¯

```bash
git add core/lexer.py tests/test_lexer.py

git commit -m "feat: implement HyperCode lexer with core 8 operations

- Tokenizes HyperCode source into token stream
- Supports core Brainfuck operations: > < + - . , [ ]
- Adds extensions: @ (2D), # (AI-native), ; (comments)
- Implements position tracking (line/column)
- 100% test coverage for lexer module

Closes: #1"

git push origin main
```

---

## ðŸ“‹ WEDNESDAY (Nov 12)

### Morning: Create Example Programs

**examples/hello_world.hc**:

```hypercode
; Hello World in HyperCode
; Demonstrates basic output

; Print 'H' (ASCII 72)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
.

; Print 'i' (ASCII 105)
>+++++++++++++++++++++++++++++++++++++++++++++++
.
```

**examples/fibonacci.hc**:

```hypercode
; Fibonacci sequence
; Shows loops and memory operations

; Initialize
>++++++++++  ; loop counter
[
    ; Fibonacci calculation here
    <+++++++++++
    >-
]
.
```

### Afternoon: Update README

**Add to README.md**:

```markdown
## ðŸš€ Quick Start

### Setup

\`\`\`bash python3 -m venv venv source venv/bin/activate pip install -r requirements.txt
\`\`\`

### Run Lexer

\`\`\`bash python core/lexer.py pytest tests/test_lexer.py -v \`\`\`

### Example Programs

\`\`\`bash

# Coming soon: compile and run examples

python -m hypercode.compiler examples/hello_world.hc -o hello.js node hello.js \`\`\`

## ðŸ“š Documentation

- [Language Spec](docs/LANGUAGE_SPEC.md)
- [API Compatibility](docs/AI_COMPAT.md)
- [Accessibility Guide](docs/ACCESSIBILITY.md)

## ðŸ¤ Contributing

See [CONTRIBUTING.md](community/CONTRIBUTING.md)
```

### Evening: Second Commit

```bash
git add examples/ README.md

git commit -m "docs: add example programs and setup instructions

- Add Hello World example
- Add Fibonacci example
- Update README with quick start
- Link to documentation"

git push origin main
```

---

## ðŸŽ¯ THURSDAY-FRIDAY (Nov 13-14)

### Parser Skeleton (Next Deliverable)

**Goal**: Create AST (Abstract Syntax Tree) from tokens

**core/parser.py** (skeleton):

```python
"""
HyperCode Parser
Converts token stream â†’ AST
"""

from dataclasses import dataclass
from typing import List, Optional
from enum import Enum
from core.lexer import Token, TokenType


class NodeType(Enum):
    """AST Node types"""
    PUSH = "push"
    POP = "pop"
    INCR = "increment"
    DECR = "decrement"
    OUTPUT = "output"
    INPUT = "input"
    LOOP = "loop"
    PROGRAM = "program"


@dataclass
class ASTNode:
    """AST Node"""
    node_type: NodeType
    value: Optional[int] = None
    children: List['ASTNode'] = None

    def __post_init__(self):
        if self.children is None:
            self.children = []


class HyperCodeParser:
    """
    Parses token stream into Abstract Syntax Tree.

    Grammar (simplified):
    program ::= (operation)*
    operation ::= PUSH | POP | INCR | DECR | OUTPUT | INPUT | LOOP
    LOOP ::= LOOP_START program LOOP_END
    """

    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.position = 0

    def parse(self) -> ASTNode:
        """Parse tokens into AST"""
        program = ASTNode(NodeType.PROGRAM)

        while self.position < len(self.tokens):
            token = self.tokens[self.position]

            if token.type == TokenType.PUSH:
                program.children.append(ASTNode(NodeType.PUSH))
                self.position += 1

            elif token.type == TokenType.POP:
                program.children.append(ASTNode(NodeType.POP))
                self.position += 1

            elif token.type == TokenType.INCR:
                program.children.append(ASTNode(NodeType.INCR))
                self.position += 1

            elif token.type == TokenType.DECR:
                program.children.append(ASTNode(NodeType.DECR))
                self.position += 1

            elif token.type == TokenType.OUTPUT:
                program.children.append(ASTNode(NodeType.OUTPUT))
                self.position += 1

            elif token.type == TokenType.INPUT:
                program.children.append(ASTNode(NodeType.INPUT))
                self.position += 1

            elif token.type == TokenType.LOOP_START:
                loop_node = self._parse_loop()
                program.children.append(loop_node)

            else:
                self.position += 1

        return program

    def _parse_loop(self) -> ASTNode:
        """Parse LOOP_START...LOOP_END block"""
        self.position += 1  # Skip LOOP_START

        loop_node = ASTNode(NodeType.LOOP)

        while (self.position < len(self.tokens) and
               self.tokens[self.position].type != TokenType.LOOP_END):
            # Recursive parse loop body
            token = self.tokens[self.position]

            if token.type == TokenType.LOOP_START:
                nested_loop = self._parse_loop()
                loop_node.children.append(nested_loop)
            else:
                # Delegate to main parse logic
                self.position -= 1  # Back up
                node = self._parse_single()
                loop_node.children.append(node)

        self.position += 1  # Skip LOOP_END
        return loop_node

    def _parse_single(self) -> ASTNode:
        """Parse single operation"""
        token = self.tokens[self.position]
        self.position += 1

        if token.type == TokenType.PUSH:
            return ASTNode(NodeType.PUSH)
        elif token.type == TokenType.POP:
            return ASTNode(NodeType.POP)
        elif token.type == TokenType.INCR:
            return ASTNode(NodeType.INCR)
        elif token.type == TokenType.DECR:
            return ASTNode(NodeType.DECR)
        elif token.type == TokenType.OUTPUT:
            return ASTNode(NodeType.OUTPUT)
        elif token.type == TokenType.INPUT:
            return ASTNode(NodeType.INPUT)
        else:
            raise ValueError(f"Unknown token: {token}")
```

**tests/test_parser.py**:

```python
"""
Unit tests for HyperCode Parser
"""

import pytest
from core.lexer import HyperCodeLexer, TokenType
from core.parser import HyperCodeParser, NodeType


class TestParserBasic:
    """Test basic parsing"""

    def test_single_operation(self):
        """Parser handles single operation"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+")

        parser = HyperCodeParser(tokens)
        ast = parser.parse()

        assert ast.node_type == NodeType.PROGRAM
        assert len(ast.children) == 1
        assert ast.children[0].node_type == NodeType.INCR

    def test_sequence(self):
        """Parser handles operation sequence"""
        lexer = HyperCodeLexer()
        tokens = lexer.tokenize("+++><.")

        parser = HyperCodeParser(tokens)
        ast = parser.parse()

        assert len(ast.children) == 6


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

### Commit Parser

```bash
git add core/parser.py tests/test_parser.py

git commit -m "feat: implement HyperCode parser with AST generation

- Convert token stream to Abstract Syntax Tree
- Support loop nesting (recursive parsing)
- AST node types for all core operations
- Parser tests for basic operations and sequences

Work towards: #2"

git push origin main
```

---

## ðŸ“Š BY END OF WEEK (Friday Nov 15)

### Milestone Checklist

- [ ] Lexer 100% working + tests passing
- [ ] Parser skeleton complete
- [ ] Example programs written
- [ ] README updated
- [ ] 3-5 commits made
- [ ] CI/CD workflows running
- [ ] GitHub Actions turning green âœ…

### Metrics

- **Code**: ~300 lines (lexer + parser)
- **Tests**: ~50 test cases
- **Commits**: 3-5 (daily discipline!)
- **GitHub Stars**: Let it happen naturally
- **Energy**: ðŸš€ HYPERFOCUS MODE ACTIVATED

---

## ðŸŽ¯ Week 2 Preview (Nov 18-22)

- **Backends**: JavaScript compilation (Week 2 focus)
- **Goal**: Parse â†’ Valid JavaScript output
- **Deliverable**: `examples/hello_world.hc` â†’ `hello.js` â†’ `"Hello World"`

---

## ðŸ”¥ YOUR POWER MOVES THIS WEEK

1. âœ… **Commit daily** (even small wins count!)
2. âœ… **Tests first** (write test, write code, watch pass)
3. âœ… **Share progress** (tweet/Discord = motivation!)
4. âœ… **Document as you go** (future you = grateful you)
5. âœ… **HYPERFOCUS** (this is your zone!)

---

## ðŸ’ª Real Talk

You got the research. You got the plan. You got the code templates.

Now? **You're in the BUILD PHASE.**

This is where legends are made, bro. ðŸ‘Š

Every commit is momentum. Every test that passes is dopamine. Every feature that works
is proof you CAN DO THIS.

**Week 1** = Foundation locked. **Month 1** = Lexer + Parser + First Backend. **Month
3** = Full alpha with AI gateway. **Month 9** = PRODUCTION.

You got this. ðŸš€

---

**Now go write that code, broski!** ðŸ’“â™¾ï¸

_November 11, 2025 | The Build Begins_
